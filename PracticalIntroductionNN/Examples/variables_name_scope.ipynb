{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>tf.Variable vs tf.get_variable</h1>\n",
    "<p style=\"font-size:20px\"><b>tf.Variable</b> is a constructor that creates new variable each time you call it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W:0\n",
      "W_1:0\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "W = tf.Variable([1,2,3],name='W')\n",
    "W1 = tf.Variable([4,5,6],name='W')\n",
    "print(W.name)\n",
    "print(W1.name)\n",
    "print(W==W1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:20px\"><b>tf.get_variable</b> will create a new variable if it hasn't been created before. If it exists, it will return an error if you try to create another one with the same name again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Variable W already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"<ipython-input-5-01bb05a3f7b9>\", line 2, in <module>\n    W = tf.get_variable('W',shape=[3],dtype=tf.float32)\n  File \"/home/sukun/anaconda3/envs/tensorflow_gpuenv/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3265, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/home/sukun/anaconda3/envs/tensorflow_gpuenv/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3183, in run_ast_nodes\n    if (yield from self.run_code(code, result)):\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-01bb05a3f7b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'W'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'W'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m   1482\u001b[0m       \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1483\u001b[0m       \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1484\u001b[0;31m       aggregation=aggregation)\n\u001b[0m\u001b[1;32m   1485\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, var_store, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m   1232\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1233\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1234\u001b[0;31m           aggregation=aggregation)\n\u001b[0m\u001b[1;32m   1235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1236\u001b[0m   def _get_partitioned_variable(self,\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m    536\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 538\u001b[0;31m           aggregation=aggregation)\n\u001b[0m\u001b[1;32m    539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m   def _get_partitioned_variable(self,\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_true_getter\u001b[0;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m    490\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 492\u001b[0;31m           aggregation=aggregation)\n\u001b[0m\u001b[1;32m    493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m     \u001b[0;31m# Set trainable value based on synchronization value.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_get_single_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m    857\u001b[0m                          \u001b[0;34m\"reuse=tf.AUTO_REUSE in VarScope? \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m                          \"Originally defined at:\\n\\n%s\" % (\n\u001b[0;32m--> 859\u001b[0;31m                              name, \"\".join(traceback.format_list(tb))))\n\u001b[0m\u001b[1;32m    860\u001b[0m       \u001b[0mfound_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfound_var\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Variable W already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"<ipython-input-5-01bb05a3f7b9>\", line 2, in <module>\n    W = tf.get_variable('W',shape=[3],dtype=tf.float32)\n  File \"/home/sukun/anaconda3/envs/tensorflow_gpuenv/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3265, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/home/sukun/anaconda3/envs/tensorflow_gpuenv/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3183, in run_ast_nodes\n    if (yield from self.run_code(code, result)):\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "W = tf.get_variable('W',shape=[3],dtype=tf.float32)\n",
    "b = tf.get_variable('W',shape=[],dtype=tf.float32)\n",
    "print(W.name)\n",
    "print(b.name)\n",
    "print(W==b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:20px\">To share or reuse the variables, we need to understand the name scopes and variable scopes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Name Scopes</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:20px\">When deal with more complex models such as neural networks, the graph can easily become cluttered with thousands of nodes. To avoid this, you can create name scopes to group related nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1:0\n",
      "my_scope/W2:0\n",
      "my_scope/add:0\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.name_scope(\"my_scope\"):\n",
    "    W1 = tf.get_variable(\"W1\",[1],dtype=tf.float32)\n",
    "    W2 = tf.Variable(1,name=\"W2\",dtype=tf.float32)\n",
    "    a = tf.add(W1,W2,name=\"add\")\n",
    "\n",
    "print(W1.name)\n",
    "print(W2.name)\n",
    "print(a.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:20px\">We can see name scope is ignored by <b>tf.get_variable</b>. The only way to place a variable accessed using <b>tf.get_variable</b> in a scope is to use a <b>variable scope</b> (We recommend everyone to use <b>variable_scope</b> and not use <b>name_scope</b> except for internal code and libraries), as in the following example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Variable Scopes</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my_scope/W1:0\n",
      "my_scope/W2:0\n",
      "my_scope/add:0\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.variable_scope(\"my_scope\"):\n",
    "    W1 = tf.get_variable(\"W1\",[1],dtype=tf.float32)\n",
    "    W2 = tf.Variable(1,name=\"W2\",dtype=tf.float32)\n",
    "    a = tf.add(W1,W2,name=\"add\")\n",
    "\n",
    "print(W1.name)\n",
    "print(W2.name)\n",
    "print(a.name)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:20px\">This allows us to easily share variables across different parts of the program, even within different name scopes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Sharing Variables</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scope_1/W:0\n",
      "scope_1/W:0\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.name_scope(\"foo\"):\n",
    "    with tf.variable_scope(\"scope_1\"):\n",
    "        W = tf.get_variable(\"W\",[1])\n",
    "with tf.name_scope(\"bar\"):\n",
    "    with tf.variable_scope(\"scope_1\",reuse=True):\n",
    "        b = tf.get_variable(\"W\",[1])\n",
    "\n",
    "assert W==b\n",
    "print(W.name)\n",
    "print(b.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>AND Gate Example</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "from datetime import datetime\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "root_logdir = \"AND_logs_scope\"\n",
    "logdir = \"{}/run-{}/\".format(root_logdir,now)\n",
    "\n",
    "#define number of training iterations\n",
    "NUM_ITER = 2000\n",
    "#define learning rate\n",
    "lr = 0.01\n",
    "\n",
    "#define the training input data\n",
    "x = np.array([[0,0],[1,0],[1,1],[0,1]],np.float32)\n",
    "#define the target outputs/labels for AND gate\n",
    "y = np.array([0,0,1,0],np.float32)\n",
    "#y = np.array([0,1,1,1],np.float32)       (uncomment this line if you are doing OR gate)\n",
    "y = np.reshape(y,[4,1])\n",
    "\n",
    "with tf.variable_scope('data'):\n",
    "    #define the placeholder to feed the training data later\n",
    "    X = tf.placeholder(tf.float32,shape=[4,2],name='X')\n",
    "    Y = tf.placeholder(tf.float32,shape=[4,1],name='Y')\n",
    "\n",
    "with tf.variable_scope(\"Parameters\"):\n",
    "    #define the weights and initialized with random normal distribution\n",
    "    W = tf.Variable(tf.random_normal([2,1]),tf.float32,name='Weight')\n",
    "    #define the bias with zero initialization\n",
    "    b = tf.Variable(tf.zeros([1,1]),tf.float32,name='bias')\n",
    "\n",
    "with tf.variable_scope(\"Prediction\"):\n",
    "    #define the predicted output\n",
    "    Y_hat = tf.add(tf.matmul(X,W),b,'Y_hat')\n",
    "with tf.variable_scope(\"Loss\"):\n",
    "    #define the loss function, we are using cross entropy function after sigmoid on Y_hat\n",
    "    loss = tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(logits=Y_hat,labels=Y),name='loss')\n",
    "\n",
    "#define the optimizer, here we use standard gradient descent method\n",
    "optimizer = tf.train.GradientDescentOptimizer(lr).minimize(loss)\n",
    "\n",
    "#initialize tensorflow variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "#!!!!!!!!!!!!!define loss scalar summary & File Writer!!!!!!!!\n",
    "with tf.name_scope('summaries'):\n",
    "    loss_summary = tf.summary.scalar('loss',loss)\n",
    "\n",
    "file_writer = tf.summary.FileWriter(logdir,tf.get_default_graph())\n",
    "\n",
    "#start tensorflow session\n",
    "with tf.Session() as sess:\n",
    "    #run initialization\n",
    "    sess.run(init)\n",
    "    #training for number of iterations\n",
    "    for i in range(NUM_ITER):\n",
    "        if i%10 == 0:\n",
    "            #fetch the loss every 10 iterations\n",
    "            summary = sess.run(loss_summary,feed_dict={X:x,Y:y})\n",
    "            #add the summary to file writer\n",
    "            file_writer.add_summary(summary,i)\n",
    "        #every iteration we run optimization on the training data X and labels Y\n",
    "        sess.run(optimizer,feed_dict = {X:x,Y:y})\n",
    "        \n",
    "    #after training, we acquire the weight and bias\n",
    "    #np.squeeze plays a role to get rid of the redundant dimension [i.e.make (2,1) be (2,)]\n",
    "    W = np.squeeze(sess.run(W))\n",
    "    b = np.squeeze(sess.run(b))\n",
    "\n",
    "#remember to close file writer!\n",
    "file_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Open Tensorboard you would see the clearer graph sctructure</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/name_scope.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
