{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PFN and PFN with mdn layer\n",
    "this note book is only for model training, see result comparison in PFN_eval.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import time as t\n",
    "import scipy.constants as spc\n",
    "import matplotlib.ticker as ticker\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_prefix = 'D:/Work/EPE/ML4pi/'\n",
    "plotpath = path_prefix+'plots/'\n",
    "modelpath_c = path_prefix+''\n",
    "modelpath = path_prefix+''\n",
    "ext_path = \"H:/EPE_file_storage/\"\n",
    "ext_modelpath = ext_path + \"Model/\"\n",
    "# ext_datapath = ext_path + \"data_storage/STMC/\"\n",
    "ext_datapath = 'H:/EPE_file_storage/data_storage/pipm/root/clusters/'\n",
    "ext_plotpath = ext_path + \"plots/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(path_prefix)\n",
    "from util import resolution_util as ru\n",
    "from util import plot_util as pu\n",
    "from util import ml_util as mu\n",
    "import uproot3 as ur\n",
    "import keras.utils.io_utils as kiu\n",
    "import h5py as h5\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training data\n",
    "with h5.File(ext_datapath+'single_cluster_only_normalized.h5', 'r') as hf:\n",
    "    data = hf['X'][:]\n",
    "    target = hf['Y'][:]\n",
    "\n",
    "# load validation data\n",
    "\"\"\"with h5.File(ext_datapath+'STMC_val.h5', 'r') as hf:\n",
    "    val_data = hf['X'][:]\n",
    "    val_target = hf['Y'][:]\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.shape)\n",
    "print(target.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use data generator feed input data to avoid memory issue, any one of the three here should work, carefully choose the generator batch sizes would improve the gpu usage rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minibatches(inputs=None, targets=None, batch_size=None, shuffle=False):\n",
    "    while True:\n",
    "        assert len(inputs) == len(targets)\n",
    "        if shuffle:\n",
    "            indices = np.arange(len(inputs))\n",
    "            np.random.shuffle(indices)\n",
    "        for start_idx in range( len(inputs) - batch_size ):\n",
    "            if shuffle:\n",
    "                excerpt = indices[start_idx:start_idx + batch_size]\n",
    "            else:\n",
    "                excerpt = slice(start_idx, start_idx + batch_size)\n",
    "            yield inputs[excerpt], targets[excerpt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(data, targets, batch_size):\n",
    "     batches = (len(data) + batch_size - 1)//batch_size\n",
    "     while True:\n",
    "          for i in range(batches):\n",
    "               X = data[i*batch_size : (i+1)*batch_size]\n",
    "               Y = targets[i*batch_size : (i+1)*batch_size]\n",
    "               yield (X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train val split\n",
    "import sklearn.model_selection as skms\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(data, target, test_size=0.2, random_state=42)\n",
    "X_val, X_test, Y_val, Y_test = train_test_split(X_val, Y_val, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ML Model setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "from tensorflow.keras.models import load_model\n",
    "import keras.backend as Kb\n",
    "import energyflow as ef\n",
    "from energyflow.archs import PFN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# network architecture parameters\n",
    "Phi_sizes, F_sizes = (100, 100, 128), (100, 100, 100)\n",
    "output_act, output_dim = 'linear', 1\n",
    "loss = 'mse'\n",
    "\n",
    "# network training parameters\n",
    "num_epoch = 1500\n",
    "batch_size = 1000\n",
    "\n",
    "netOpt = tf.keras.optimizers.Adam(\n",
    "    learning_rate=.002,\n",
    "    beta_1=0.9,\n",
    "    beta_2=0.999,\n",
    "    epsilon=1e-09,\n",
    "    amsgrad=False)\n",
    "\n",
    "RMS_prop = tf.keras.optimizers.RMSprop(learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# network architecture parameters\n",
    "Phi_sizes, F_sizes = (100, 100, 128), (100, 100, 100)\n",
    "output_act, output_dim = 'linear', 1\n",
    "loss = 'mse'\n",
    "\n",
    "Kb.clear_session()\n",
    "\n",
    "pfn = PFN(input_dim=4, Phi_sizes=Phi_sizes, F_sizes=F_sizes, \n",
    "          output_act=output_act, output_dim=output_dim, loss=loss,\n",
    "          optimizer=netOpt, metrics=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'pfn_cluster_only_dg'\n",
    "chkpoint = tf.keras.callbacks.ModelCheckpoint('model/' + model_name + '.h5', monitor='val_loss', verbose=1, save_best_only=True)\n",
    "history_logger = tf.keras.callbacks.CSVLogger('log/' + model_name + '.log', separator=',', append=True)\n",
    "t0 = t.time()\n",
    "\"\"\"history_ct = pfn.fit(X_train, Y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=500,\n",
    "                    validation_data=(X_val, Y_val),\n",
    "                    callbacks=[chkpoint, history_logger])\"\"\"\n",
    "\n",
    "history_ct = pfn.fit(data_generator(X_train, Y_train, batch_size=2048),\n",
    "                    steps_per_epoch=len(data)//2048,\n",
    "                    validation_data=(data_generator(X_val, Y_val, batch_size=2048)),\n",
    "                    validation_steps=len(X_train)//2048,\n",
    "                    epochs=1000,\n",
    "                    verbose=1,\n",
    "                    callbacks=[chkpoint, history_logger])\n",
    "t1 = t.time()\n",
    "print('Time to train: '+str(t1-t0)+' (s)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "tf.config.experimental.set_memory_growth(physical_gpus[0],True)\n",
    "logical_gpus = tf.config.list_logical_devices(\"GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('pfn_regressor_30f_GN.log.npy',history_ct.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Time to train: '+str(t1-t0)+' (s)')\n",
    "print('Time to train: '+str((t1-t0)/60)+' (m)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6,6))\n",
    "plt.plot(history_ct.history['val_loss'], label=\"Validation\",linestyle='dashed')\n",
    "plt.plot(history_ct.history['loss'], label=\"Training\")\n",
    "plt.yscale('log')\n",
    "plt.ylim(.001,10)\n",
    "plt.yticks(fontsize=13)\n",
    "plt.xlim(0,1000)\n",
    "plt.xticks(fontsize=13)\n",
    "plt.xlabel('Epochs', fontsize=14)\n",
    "plt.ylabel('Loss', fontsize=14)\n",
    "plt.legend(loc='upper right', ncol=1)\n",
    "plt.text(1000, 1.5, 'LR=2e-3', fontsize=13)\n",
    "plt.text(1000, 1, 'Epoch: 1000', fontsize=13)\n",
    "# plt.tight_layout()\n",
    "# plt.savefig('Regression_Plots/July/XY_STSC_lossCurves_3000batch_LR1e-2_2021-07-016.png', format='png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "# from . import plot_util as pu\n",
    "from util import plot_util as pu\n",
    "\n",
    "def _iqrOverMed(x):\n",
    "    # get the IQR via the percentile function\n",
    "    # 84 is median + 1 sigma, 16 is median - 1 sigma\n",
    "    q16, q84 = np.percentile(x, [16, 84])\n",
    "    return (q84 - q16) / (2 * np.median(x))\n",
    "\n",
    "def resolutionPlot(x, y, figfile='',\n",
    "                   xlabel='True Energy [GeV]', ylabel='Response IQR / (2 x Median)',\n",
    "                   atlas_x=-1, atlas_y=-1, simulation=False,\n",
    "                   xlim=(0.3,1000), ylim=(0,1), \n",
    "                   textlist=[]):\n",
    "    xbin = [10**exp for exp in  np.arange(-1.0, 3.1, 0.1)]\n",
    "    xcenter = [(xbin[i] + xbin[i+1]) / 2 for i in range(len(xbin)-1)]\n",
    "\n",
    "    resolution = stats.binned_statistic(x, y, bins=xbin, statistic=_iqrOverMed).statistic\n",
    "    \n",
    "    plt.cla(); plt.clf()\n",
    "    fig = plt.figure()\n",
    "    fig.patch.set_facecolor('white')\n",
    "    plt.plot(xcenter, resolution)\n",
    "    plt.xscale('log')\n",
    "    plt.xlim(xlim)\n",
    "    plt.ylim(ylim)\n",
    "    pu.ampl.set_xlabel(xlabel)\n",
    "    pu.ampl.set_ylabel(ylabel)\n",
    "\n",
    "    pu.drawLabels(fig, atlas_x, atlas_y, simulation, textlist)\n",
    "\n",
    "    if figfile != '':\n",
    "        plt.savefig(figfile)\n",
    "    plt.show()\n",
    "\n",
    "    return xcenter, resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_probability as tfp\n",
    "\n",
    "def point_mask_fn(X, mask_val=0.):\n",
    "    return Kb.cast(Kb.any(Kb.not_equal(X, mask_val), axis=-1), Kb.dtype(X))\n",
    "\n",
    "def convert_to_tensor(X):\n",
    "    return tf.concat([tfp.distributions.Distribution.mean(X), tfp.distributions.Distribution.stddev(X)],1)\n",
    "\n",
    "def ParticleFlow_MDN(num_features, name=\"PFN_MDN_Network\"):\n",
    "    \n",
    "    event_shape = [1]\n",
    "    num_components = 1\n",
    "    params_size = tfp.layers.MixtureNormal.params_size(num_components, event_shape)\n",
    "\n",
    "\n",
    "    inputs = keras.Input(shape=(None, num_features), name='input')\n",
    "\n",
    "    dense_0 = layers.Dense(100)\n",
    "    t_dist_0 = layers.TimeDistributed(dense_0, name='t_dist_0')(inputs)\n",
    "    activation_0 = layers.Activation('relu', name=\"activation_0\")(t_dist_0)\n",
    "    \n",
    "    dense_1 = layers.Dense(100)\n",
    "    t_dist_1 = layers.TimeDistributed(dense_1, name='t_dist_1')(activation_0)\n",
    "    activation_1 = layers.Activation('relu', name='activation_1')(t_dist_1)\n",
    "    \n",
    "    dense_2 = layers.Dense(128)\n",
    "    t_dist_2 = layers.TimeDistributed(dense_2, name='t_dist_2')(activation_1)\n",
    "    activation_2 = layers.Activation('relu', name='activation_2')(t_dist_2)\n",
    "    \n",
    "    lambda_layer = layers.Lambda(point_mask_fn, output_shape=(None, None),\n",
    "                                mask=None,\n",
    "                                name='mask')(inputs)\n",
    "\n",
    "    sum_layer = layers.Dot(axes=(1,1), name='sum')([lambda_layer, activation_2])\n",
    "    \n",
    "    dense_3 = layers.Dense(100, name='dense_0')(sum_layer)\n",
    "    activation_3 = layers.Activation('relu', name=\"activation_3\")(dense_3)\n",
    "    \n",
    "    dense_4 = layers.Dense(100, name='dense_1')(activation_3)\n",
    "    activation_4 = layers.Activation('relu', name=\"activation_4\")(dense_4)\n",
    "    \n",
    "    dense_5 = layers.Dense(100, name='dense_2')(activation_4)\n",
    "    activation_5 = layers.Activation('relu', name=\"activation_5\")(dense_5)\n",
    "    \n",
    "    dense_6 = layers.Dense(units=params_size, activation=lambda x: tf.clip_by_value(x, -30., 30.))(activation_5)\n",
    "\n",
    "    \n",
    "    mdn_0 = tfp.layers.MixtureNormal(num_components, event_shape, validate_args=True,\n",
    "                                          convert_to_tensor_fn=convert_to_tensor)(dense_6)\n",
    "    \n",
    "    return keras.Model(inputs=inputs, outputs=mdn_0, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Kb.clear_session()\n",
    "\n",
    "netOpt = tf.keras.optimizers.Adam(\n",
    "    learning_rate=.002,\n",
    "    beta_1=0.9,\n",
    "    beta_2=0.999,\n",
    "    epsilon=1e-09,\n",
    "    amsgrad=False)\n",
    "\n",
    "PFN_mdn = ParticleFlow_MDN(num_features=4)\n",
    "PFN_mdn.compile(optimizer = netOpt, loss=lambda y, p_y: -p_y.log_prob(y))\n",
    "PFN_mdn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'PFN_MDN_CL'\n",
    "chkpoint = tf.keras.callbacks.ModelCheckpoint('model/' + model_name + '.h5', monitor='val_loss', verbose=1, save_best_only=True)\n",
    "# earlystop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=1e-4, patience=150, verbose=0, restore_best_weights=True)\n",
    "\n",
    "# save history to csv, or to npy later, in order to plot comparison of multiple training runs\n",
    "history_logger = tf.keras.callbacks.CSVLogger('log/' + model_name + '.log', separator=',', append=True)\n",
    "\n",
    "# load previous model if it exists\n",
    "if os.path.isfile('model/' + model_name + '.h5'):\n",
    "    PFN_mdn.load_weights('model/' + model_name + '.h5')\n",
    "\n",
    "history_ct = PFN_mdn.fit(X_train, Y_train,\n",
    "                    batch_size=800,\n",
    "                    epochs=500,\n",
    "                    validation_data=(X_val, Y_val),\n",
    "                    callbacks=[chkpoint, history_logger])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture output\n",
    "model_name = 'pfn_mdn_regressor_1'\n",
    "chkpoint = tf.keras.callbacks.ModelCheckpoint('model/' + model_name + '.h5', monitor='val_loss', verbose=1, save_best_only=True)\n",
    "# earlystop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=1e-4, patience=150, verbose=0, restore_best_weights=True)\n",
    "\n",
    "# save history to csv, or to npy later, in order to plot comparison of multiple training runs\n",
    "history_logger = tf.keras.callbacks.CSVLogger('log/' + model_name + '.log', separator=',', append=True)\n",
    "\n",
    "# back up\n",
    "# backup  = tf.keras.callbacks.BackupAndRestore(backup_dir=\"backup\")\n",
    "\n",
    "t0 = t.time()\n",
    "history_ct = PFN_mdn.fit(data_generator(data, target, batch_size=1024),\n",
    "        epochs=1000,\n",
    "        steps_per_epoch=len(data)//1024,\n",
    "        validation_data=data_generator(val_data, val_target, batch_size=800),\n",
    "        validation_steps=len(val_data)//800,\n",
    "        verbose=1,\n",
    "        callbacks=[chkpoint,history_logger])\n",
    "t1 = t.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pfn_history = np.load(ext_modelpath + 'pfn_regressor_3f_1.log.npy',allow_pickle='TRUE').item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('PFN_MDN_regressor_weight_nl.log.npy',history_ct.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Time to train: '+str(t1-t0)+' (s)')\n",
    "print('Time to train: '+str((t1-t0)/60)+' (m)')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b5d44aea0cc164fd0ebfc1732b90238ce53cef8d0e631d031394e4c6aaa004a9"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('tensorflow-gpu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
