{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PFN and PFN with mdn layer\n",
    "this note book is only for model training, see result comparison in PFN_eval.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import time as t\n",
    "import scipy.constants as spc\n",
    "import matplotlib.ticker as ticker\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_prefix = 'D:/Work/EPE/ML4pi/'\n",
    "plotpath = path_prefix+'plots/'\n",
    "modelpath_c = path_prefix+''\n",
    "modelpath = path_prefix+''\n",
    "ext_path = \"H:/EPE_file_storage/\"\n",
    "ext_modelpath = ext_path + \"Model/\"\n",
    "# ext_datapath = ext_path + \"data_storage/STMC/\"\n",
    "ext_datapath = 'D:/Work/Datastorage/STMC/'\n",
    "ext_plotpath = ext_path + \"plots/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(path_prefix)\n",
    "from util import resolution_util as ru\n",
    "from util import plot_util as pu\n",
    "from util import ml_util as mu\n",
    "import uproot3 as ur\n",
    "import keras.utils.io_utils as kiu\n",
    "import h5py as h5\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Datagenerator(tf.keras.utils.Sequence):\n",
    "    'generate data for keras model'\n",
    "    def __init__(self, data_path, batch_size, shuffle=True):\n",
    "        self.data_path = data_path\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self._readhdf5()\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def _readhdf5(self):\n",
    "        'read hdf5 file'\n",
    "        with h5.File(self.data_path, 'r') as hf:\n",
    "            self.data = hf['X'][:]\n",
    "            self.target = hf['Y'][:]\n",
    "\n",
    "    def __len__(self):\n",
    "        'return number of batches per epoch'\n",
    "        return int(np.floor(len(self.data) / self.batch_size))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        'generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        # Find list of IDs\n",
    "        X, Y = self.__data_generation\n",
    "        return X, Y\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        'update index after each epoch'\n",
    "        self.indexes = np.arange(len(self.data))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "    \n",
    "    def __data_generation(self, X, Y):\n",
    "        'generate data containing batch_size samples'\n",
    "        # Initialization\n",
    "        X_batch = np.empty((self.batch_size, *X.shape))\n",
    "        Y_batch = np.empty((self.batch_size, *Y.shape))\n",
    "        # Generate data\n",
    "        for i, ID in enumerate(X):\n",
    "            X_batch[i,:,:,:] = X[ID]\n",
    "            Y_batch[i,] = Y[ID]\n",
    "        return X_batch, Y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_generator = Datagenerator(ext_datapath+'STMC_full.h5', batch_size=512, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(data_path):\n",
    "    'read hdf5 file'\n",
    "    with h5.File(data_path, 'r') as hf:\n",
    "        X = hf['X'][:]\n",
    "        Y = hf['Y'][:]\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training data\n",
    "with h5.File(ext_datapath+'STMC_full_1.h5', 'r') as hf:\n",
    "    data = hf['X'][:]\n",
    "    target = hf['Y'][:]\n",
    "\n",
    "# load validation data\n",
    "with h5.File(ext_datapath+'STMC_val.h5', 'r') as hf:\n",
    "    val_data = hf['X'][:]\n",
    "    val_target = hf['Y'][:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use data generator feed input data to avoid memory issue, any one of the three here should work, carefully choose the generator batch sizes would improve the gpu usage rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minibatches(inputs=None, targets=None, batch_size=None, shuffle=False, aug=None):\n",
    "    while True:\n",
    "        assert len(inputs) == len(targets)\n",
    "        if shuffle:\n",
    "            indices = np.arange(len(inputs))\n",
    "            np.random.shuffle(indices)\n",
    "        for start_idx in range( len(inputs) - batch_size ):\n",
    "            if shuffle:\n",
    "                excerpt = indices[start_idx:start_idx + batch_size]\n",
    "                if aug is not None:\n",
    "                  (inputs[excerpt], targets[excerpt]) = next(aug.flow(inputs[excerpt],targets[excerpt], batch_size=batch_size))\n",
    "            else:\n",
    "                excerpt = slice(start_idx, start_idx + batch_size)\n",
    "                if aug is not None:\n",
    "                  (inputs[excerpt], targets[excerpt]) = next(aug.flow(inputs[excerpt],targets[excerpt], batch_size=batch_size))\n",
    "            yield inputs[excerpt], targets[excerpt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(data, targets, batch_size):\n",
    "     batches = (len(data) + batch_size - 1)//batch_size\n",
    "     while True:\n",
    "          for i in range(batches):\n",
    "               X = data[i*batch_size : (i+1)*batch_size]\n",
    "               Y = targets[i*batch_size : (i+1)*batch_size]\n",
    "               yield (X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write a tensorflow data input pipline to load data batches by batches to tensorflow model to avoid memory issues\n",
    "def data_generator(x_data, y_data, batch_size):\n",
    "    # generate a batch of data\n",
    "    while True:\n",
    "        for i in range(0, len(x_data), batch_size):\n",
    "            yield x_data[i:i+batch_size], y_data[i:i+batch_size]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ML Model setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "from tensorflow.keras.models import load_model\n",
    "import keras.backend as Kb\n",
    "import energyflow as ef\n",
    "from energyflow.archs import PFN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpu memory usage\n",
    "gpus = tf.config.experimental.list_physical_devices(device_type='GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# network architecture parameters\n",
    "Phi_sizes, F_sizes = (100, 100, 128), (100, 100, 100)\n",
    "output_act, output_dim = 'linear', 1\n",
    "loss = 'mse'\n",
    "\n",
    "# network training parameters\n",
    "num_epoch = 1500\n",
    "batch_size = 1000\n",
    "\n",
    "netOpt = tf.keras.optimizers.Adam(\n",
    "    learning_rate=.002,\n",
    "    beta_1=0.9,\n",
    "    beta_2=0.999,\n",
    "    epsilon=1e-09,\n",
    "    amsgrad=False)\n",
    "\n",
    "RMS_prop = tf.keras.optimizers.RMSprop(learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input (InputLayer)              [(None, None, 5)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tdist_0 (TimeDistributed)       (None, None, 100)    600         input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, None, 100)    0           tdist_0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tdist_1 (TimeDistributed)       (None, None, 100)    10100       activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, None, 100)    0           tdist_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tdist_2 (TimeDistributed)       (None, None, 128)    12928       activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "mask (Lambda)                   (None, None)         0           input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, None, 128)    0           tdist_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sum (Dot)                       (None, 128)          0           mask[0][0]                       \n",
      "                                                                 activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_0 (Dense)                 (None, 100)          12900       sum[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 100)          0           dense_0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 100)          10100       activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 100)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 100)          10100       activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 100)          0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "output (Dense)                  (None, 1)            101         activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 1)            0           output[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 56,829\n",
      "Trainable params: 56,829\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# network architecture parameters\n",
    "Phi_sizes, F_sizes = (100, 100, 128), (100, 100, 100)\n",
    "output_act, output_dim = 'linear', 1\n",
    "loss = 'mse'\n",
    "\n",
    "Kb.clear_session()\n",
    "\n",
    "pfn = PFN(input_dim=5, Phi_sizes=Phi_sizes, F_sizes=F_sizes, \n",
    "          output_act=output_act, output_dim=output_dim, loss=loss,\n",
    "          optimizer=netOpt, metrics=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chkpoint = tf.keras.callbacks.ModelCheckpoint(ext_modelpath + 'pfn_regressor_30f_GN.h5', monitor='val_loss', verbose=1, save_best_only=True)\n",
    "t0 = t.time()\n",
    "history_ct = pfn.train_on_batch(data, target, batch_size=512,\n",
    "                    epochs=num_epoch,\n",
    "                    verbose=1)\n",
    "t1 = t.time()\n",
    "print('Time to train: '+str(t1-t0)+' (s)')\n",
    "print('Time to train: '+str(t1-t0)/60+' (m)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "tf.config.experimental.set_memory_growth(physical_gpus[0],True)\n",
    "logical_gpus = tf.config.list_logical_devices(\"GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chkpoint = tf.keras.callbacks.ModelCheckpoint(ext_modelpath + 'pfn_regressor_30f.h5', monitor='val_loss', verbose=1, save_best_only=True)\n",
    "# earlystop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=1e-4, patience=150, verbose=0, restore_best_weights=True)\n",
    "\n",
    "# save history to csv, or to npy later, in order to plot comparison of multiple training runs\n",
    "# history_logger = tf.keras.callbakcs.CSVLogger(ext_modelpath + 'pfn_regressor_3f_1.log', separator=',', append=True)\n",
    "\n",
    "t0 = t.time()\n",
    "history_ct = pfn.fit(data, target,\n",
    "        epochs=1000,\n",
    "        batch_size=20,\n",
    "        validation_split=0.2,\n",
    "        verbose=1,\n",
    "        callbacks=[chkpoint])\n",
    "t1 = t.time()\n",
    "print('Time to train: '+str(t1-t0)+' (s)')\n",
    "print('Time to train: '+str(t1-t0)/60+' (m)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(ext_modelpath +'pfn_regressor_3f_nl.log.npy',history_ct.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Time to train: '+str(t1-t0)+' (s)')\n",
    "print('Time to train: '+str((t1-t0)/60)+' (m)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6,6))\n",
    "plt.plot(history_ct.history['val_loss'], label=\"Validation\",linestyle='dashed')\n",
    "plt.plot(history_ct.history['loss'], label=\"Training\")\n",
    "plt.yscale('log')\n",
    "plt.ylim(.001,10)\n",
    "plt.yticks(fontsize=13)\n",
    "plt.xlim(0,1000)\n",
    "plt.xticks(fontsize=13)\n",
    "plt.xlabel('Epochs', fontsize=14)\n",
    "plt.ylabel('Loss', fontsize=14)\n",
    "plt.legend(loc='upper right', ncol=1)\n",
    "plt.text(1000, 1.5, 'LR=2e-3', fontsize=13)\n",
    "plt.text(1000, 1, 'Epoch: 1000', fontsize=13)\n",
    "# plt.tight_layout()\n",
    "# plt.savefig('Regression_Plots/July/XY_STSC_lossCurves_3000batch_LR1e-2_2021-07-016.png', format='png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "# from . import plot_util as pu\n",
    "from util import plot_util as pu\n",
    "\n",
    "def _iqrOverMed(x):\n",
    "    # get the IQR via the percentile function\n",
    "    # 84 is median + 1 sigma, 16 is median - 1 sigma\n",
    "    q16, q84 = np.percentile(x, [16, 84])\n",
    "    return (q84 - q16) / (2 * np.median(x))\n",
    "\n",
    "def resolutionPlot(x, y, figfile='',\n",
    "                   xlabel='True Energy [GeV]', ylabel='Response IQR / (2 x Median)',\n",
    "                   atlas_x=-1, atlas_y=-1, simulation=False,\n",
    "                   xlim=(0.3,1000), ylim=(0,1), \n",
    "                   textlist=[]):\n",
    "    xbin = [10**exp for exp in  np.arange(-1.0, 3.1, 0.1)]\n",
    "    xcenter = [(xbin[i] + xbin[i+1]) / 2 for i in range(len(xbin)-1)]\n",
    "\n",
    "    resolution = stats.binned_statistic(x, y, bins=xbin, statistic=_iqrOverMed).statistic\n",
    "    \n",
    "    plt.cla(); plt.clf()\n",
    "    fig = plt.figure()\n",
    "    fig.patch.set_facecolor('white')\n",
    "    plt.plot(xcenter, resolution)\n",
    "    plt.xscale('log')\n",
    "    plt.xlim(xlim)\n",
    "    plt.ylim(ylim)\n",
    "    pu.ampl.set_xlabel(xlabel)\n",
    "    pu.ampl.set_ylabel(ylabel)\n",
    "\n",
    "    pu.drawLabels(fig, atlas_x, atlas_y, simulation, textlist)\n",
    "\n",
    "    if figfile != '':\n",
    "        plt.savefig(figfile)\n",
    "    plt.show()\n",
    "\n",
    "    return xcenter, resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_probability as tfp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def point_mask_fn(X, mask_val=0.):\n",
    "    return Kb.cast(Kb.any(Kb.not_equal(X, mask_val), axis=-1), Kb.dtype(X))\n",
    "\n",
    "def convert_to_tensor(X):\n",
    "    return tf.concat([tfp.distributions.Distribution.mean(X), tfp.distributions.Distribution.stddev(X)],1)\n",
    "\n",
    "def ParticleFlow_MDN(num_features, name=\"PFN_MDN_Network\"):\n",
    "    \n",
    "    event_shape = [1]\n",
    "    num_components = 3\n",
    "    params_size = tfp.layers.MixtureNormal.params_size(num_components, event_shape)\n",
    "\n",
    "\n",
    "    inputs = keras.Input(shape=(None, num_features), name='input')\n",
    "\n",
    "    dense_0 = layers.Dense(100)\n",
    "    t_dist_0 = layers.TimeDistributed(dense_0, name='t_dist_0')(inputs)\n",
    "    activation_0 = layers.Activation('relu', name=\"activation_0\")(t_dist_0)\n",
    "    \n",
    "    dense_1 = layers.Dense(100)\n",
    "    t_dist_1 = layers.TimeDistributed(dense_1, name='t_dist_1')(activation_0)\n",
    "    activation_1 = layers.Activation('relu', name='activation_1')(t_dist_1)\n",
    "    \n",
    "    dense_2 = layers.Dense(128)\n",
    "    t_dist_2 = layers.TimeDistributed(dense_2, name='t_dist_2')(activation_1)\n",
    "    activation_2 = layers.Activation('relu', name='activation_2')(t_dist_2)\n",
    "    \n",
    "    lambda_layer = layers.Lambda(point_mask_fn, output_shape=(None, None),\n",
    "                                mask=None,\n",
    "                                name='mask')(inputs)\n",
    "\n",
    "    sum_layer = layers.Dot(axes=(1,1), name='sum')([lambda_layer, activation_2])\n",
    "    \n",
    "    dense_3 = layers.Dense(100, name='dense_0')(sum_layer)\n",
    "    activation_3 = layers.Activation('relu', name=\"activation_3\")(dense_3)\n",
    "    \n",
    "    dense_4 = layers.Dense(100, name='dense_1')(activation_3)\n",
    "    activation_4 = layers.Activation('relu', name=\"activation_4\")(dense_4)\n",
    "    \n",
    "    dense_5 = layers.Dense(100, name='dense_2')(activation_4)\n",
    "    activation_5 = layers.Activation('relu', name=\"activation_5\")(dense_5)\n",
    "    \n",
    "    dense_6 = layers.Dense(units=params_size, activation=lambda x: tf.clip_by_value(x, -30., 30.))(activation_5)\n",
    "\n",
    "    \n",
    "    mdn_0 = tfp.layers.MixtureNormal(num_components, event_shape, validate_args=True,\n",
    "                                          convert_to_tensor_fn=convert_to_tensor)(dense_6)\n",
    "    \n",
    "    return keras.Model(inputs=inputs, outputs=mdn_0, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"PFN_MDN_Network\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input (InputLayer)              [(None, None, 5)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "t_dist_0 (TimeDistributed)      (None, None, 100)    600         input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_0 (Activation)       (None, None, 100)    0           t_dist_0[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "t_dist_1 (TimeDistributed)      (None, None, 100)    10100       activation_0[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, None, 100)    0           t_dist_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "t_dist_2 (TimeDistributed)      (None, None, 128)    12928       activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "mask (Lambda)                   (None, None)         0           input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, None, 128)    0           t_dist_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "sum (Dot)                       (None, 128)          0           mask[0][0]                       \n",
      "                                                                 activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_0 (Dense)                 (None, 100)          12900       sum[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 100)          0           dense_0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 100)          10100       activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 100)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 100)          10100       activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 100)          0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 9)            909         activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "mixture_normal (MixtureNormal)  multiple             0           dense_3[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 57,637\n",
      "Trainable params: 57,637\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "Kb.clear_session()\n",
    "\n",
    "netOpt = tf.keras.optimizers.Adam(\n",
    "    learning_rate=.002,\n",
    "    beta_1=0.9,\n",
    "    beta_2=0.999,\n",
    "    epsilon=1e-09,\n",
    "    amsgrad=False)\n",
    "\n",
    "PFN_mdn = ParticleFlow_MDN(num_features=5)\n",
    "PFN_mdn.compile(optimizer = netOpt, loss=lambda y, p_y: -p_y.log_prob(y))\n",
    "PFN_mdn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "623/623 [==============================] - 80s 127ms/step - loss: 186.1223 - val_loss: 2.3685\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 2.36851, saving model to H:/EPE_file_storage/Model\\PFN_MDN_regressor_weight_dg.h5\n",
      "Epoch 2/200\n",
      "623/623 [==============================] - 78s 126ms/step - loss: 2.6546 - val_loss: 3.7507\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 2.36851\n",
      "Epoch 3/200\n",
      "623/623 [==============================] - 78s 125ms/step - loss: 1.7416 - val_loss: 0.2406\n",
      "\n",
      "Epoch 00003: val_loss improved from 2.36851 to 0.24060, saving model to H:/EPE_file_storage/Model\\PFN_MDN_regressor_weight_dg.h5\n",
      "Epoch 4/200\n",
      "623/623 [==============================] - 77s 124ms/step - loss: 0.0069 - val_loss: -0.6196\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.24060 to -0.61964, saving model to H:/EPE_file_storage/Model\\PFN_MDN_regressor_weight_dg.h5\n",
      "Epoch 5/200\n",
      "623/623 [==============================] - 79s 126ms/step - loss: -0.4770 - val_loss: -0.7458\n",
      "\n",
      "Epoch 00005: val_loss improved from -0.61964 to -0.74580, saving model to H:/EPE_file_storage/Model\\PFN_MDN_regressor_weight_dg.h5\n",
      "Epoch 6/200\n",
      "623/623 [==============================] - 79s 126ms/step - loss: -0.8506 - val_loss: -1.0293\n",
      "\n",
      "Epoch 00006: val_loss improved from -0.74580 to -1.02933, saving model to H:/EPE_file_storage/Model\\PFN_MDN_regressor_weight_dg.h5\n",
      "Epoch 7/200\n",
      "623/623 [==============================] - 79s 127ms/step - loss: -0.8222 - val_loss: -0.6609\n",
      "\n",
      "Epoch 00007: val_loss did not improve from -1.02933\n",
      "Epoch 8/200\n",
      "623/623 [==============================] - 80s 128ms/step - loss: -0.9650 - val_loss: -0.4933\n",
      "\n",
      "Epoch 00008: val_loss did not improve from -1.02933\n",
      "Epoch 9/200\n",
      "623/623 [==============================] - 79s 127ms/step - loss: -1.1354 - val_loss: -0.8177\n",
      "\n",
      "Epoch 00009: val_loss did not improve from -1.02933\n",
      "Epoch 10/200\n",
      "623/623 [==============================] - 79s 127ms/step - loss: -1.1227 - val_loss: -1.2588\n",
      "\n",
      "Epoch 00010: val_loss improved from -1.02933 to -1.25880, saving model to H:/EPE_file_storage/Model\\PFN_MDN_regressor_weight_dg.h5\n",
      "Epoch 11/200\n",
      "623/623 [==============================] - 79s 127ms/step - loss: -1.2206 - val_loss: -1.1118\n",
      "\n",
      "Epoch 00011: val_loss did not improve from -1.25880\n",
      "Epoch 12/200\n",
      "623/623 [==============================] - 84s 135ms/step - loss: -1.1968 - val_loss: -1.0638\n",
      "\n",
      "Epoch 00012: val_loss did not improve from -1.25880\n",
      "Epoch 13/200\n",
      "623/623 [==============================] - 82s 132ms/step - loss: -1.2230 - val_loss: -1.1835\n",
      "\n",
      "Epoch 00013: val_loss did not improve from -1.25880\n",
      "Epoch 14/200\n",
      "623/623 [==============================] - 83s 133ms/step - loss: -1.3735 - val_loss: -0.9824\n",
      "\n",
      "Epoch 00014: val_loss did not improve from -1.25880\n",
      "Epoch 15/200\n",
      "623/623 [==============================] - 83s 133ms/step - loss: -1.2728 - val_loss: -1.3019\n",
      "\n",
      "Epoch 00015: val_loss improved from -1.25880 to -1.30188, saving model to H:/EPE_file_storage/Model\\PFN_MDN_regressor_weight_dg.h5\n",
      "Epoch 16/200\n",
      "623/623 [==============================] - 83s 133ms/step - loss: -1.3698 - val_loss: -1.2751\n",
      "\n",
      "Epoch 00016: val_loss did not improve from -1.30188\n",
      "Epoch 17/200\n",
      "623/623 [==============================] - 82s 132ms/step - loss: -1.4449 - val_loss: -1.2502\n",
      "\n",
      "Epoch 00017: val_loss did not improve from -1.30188\n",
      "Epoch 18/200\n",
      "204/623 [========>.....................] - ETA: 52s - loss: -1.3743"
     ]
    }
   ],
   "source": [
    "chkpoint = tf.keras.callbacks.ModelCheckpoint(ext_modelpath + 'PFN_MDN_regressor_weight_dg.h5', monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=True)\n",
    "# earlystop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=1e-4, patience=150, verbose=0, restore_best_weights=True)\n",
    "\n",
    "t0 = t.time()\n",
    "history_ct = PFN_mdn.fit(minibatches(data, target, batch_size=2048),\n",
    "        epochs=200,\n",
    "        steps_per_epoch=len(data)//2048,\n",
    "        validation_data=minibatches(val_data, val_target, batch_size=1024),\n",
    "        validation_steps=len(val_data)//1024,\n",
    "        verbose=1,\n",
    "        callbacks=[chkpoint])\n",
    "t1 = t.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pfn_history = np.load(ext_modelpath + 'pfn_regressor_3f_1.log.npy',allow_pickle='TRUE').item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(ext_modelpath + 'PFN_MDN_regressor_weight_nl.log.npy',history_ct.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Time to train: '+str(t1-t0)+' (s)')\n",
    "print('Time to train: '+str((t1-t0)/60)+' (m)')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b5d44aea0cc164fd0ebfc1732b90238ce53cef8d0e631d031394e4c6aaa004a9"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('tensorflow-gpu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
