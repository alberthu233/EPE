{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PFN model load data and normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import time as t\n",
    "import scipy.constants as spc\n",
    "import matplotlib.ticker as ticker\n",
    "import h5py as h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_prefix = 'D:/Work/EPE/ML4pi/'\n",
    "plotpath = path_prefix+'plots/'\n",
    "modelpath_c = path_prefix+''\n",
    "modelpath = path_prefix+''\n",
    "ext_path = \"H:/EPE_file_storage/\"\n",
    "ext_modelpath = ext_path + \"Model/\"\n",
    "# ext_datapath = ext_path + \"data_storage/STMC/\"\n",
    "ext_datapath = 'D:/Work/Datastorage/STMC/'\n",
    "ext_plotpath = ext_path + \"plots/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(path_prefix)\n",
    "from util import resolution_util as ru\n",
    "from util import plot_util as pu\n",
    "from util import ml_util as mu\n",
    "import uproot3 as ur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tvt_num(_data, _tvt=(75, 10, 15)):\n",
    "    ''' Function designed to output appropriate numbers for traning validation and testing given\n",
    "    a variable length input. TVT expressed as ratios and do not need to add to 100. '''\n",
    "    _tot = len(_data)\n",
    "    _train, _val, _test = _tvt\n",
    "    _tvt_sum = _train + _val + _test\n",
    "    \n",
    "    _train_rtrn = round(_train*_tot/_tvt_sum)\n",
    "    _val_rtrn = round(_val*_tot/_tvt_sum)\n",
    "    _test_rtrn = _tot - _train_rtrn - _val_rtrn\n",
    "    \n",
    "    return _train_rtrn, _val_rtrn, _test_rtrn\n",
    "\n",
    "def normalize_input1d(arr, mask):\n",
    "    ''' Note that non masked values are left alone i.e. zeros if mask = arr != 0'''\n",
    "    len_arr = arr.shape[0]\n",
    "    mean = np.repeat(np.mean(arr, where=mask), len_arr)\n",
    "    std_dev = np.repeat(np.std(arr, where=mask), len_arr)\n",
    "    norm_arr = np.subtract(arr, mean, out=arr, where=mask)\n",
    "    std_mask = np.logical_and(std_dev!=0, mask)\n",
    "    norm_arr = np.divide(norm_arr, std_dev, out=norm_arr, where=std_mask)\n",
    "    return norm_arr\n",
    "\n",
    "def normalize_input2d(arr, mask):\n",
    "    ''' Truth value is where to perform the operation, exclude False vals.\n",
    "    returns: Array with the shape of arr with normalization carried out with mask '''\n",
    "    len_ax1 = arr.shape[1]\n",
    "    mean = np.tile(np.mean(arr, axis=1, where=mask), (len_ax1,1)).transpose()\n",
    "    std_dev = np.tile(np.std(arr, axis=1, where=mask), (len_ax1,1)).transpose()\n",
    "    norm_arr = np.subtract(arr, mean, out=arr, where=mask)\n",
    "    std_mask = np.logical_and(std_dev != 0, mask)\n",
    "    norm_arr = np.divide(norm_arr, std_dev, out=norm_arr, where=std_mask)\n",
    "    return norm_arr\n",
    "\n",
    "def eval_generator(data, batch_size):\n",
    "     batches = (len(data) + batch_size - 1)//batch_size\n",
    "     for i in range(batches):\n",
    "          X = data[i*batch_size : (i+1)*batch_size]\n",
    "          yield (X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalized_data(X,Y):\n",
    "    ''' \n",
    "    Normalizes the data\n",
    "    '''\n",
    "    t0 = t.time()\n",
    "    # take log of Y\n",
    "    target_zero_mask = Y[:,0] > .05\n",
    "    Ylog = np.log(Y[:,0][target_zero_mask])\n",
    "    \n",
    "    ## Normalize rPerp to 1/3630\n",
    "    rPerp_mask = X[:,:,3] != 0\n",
    "    X[:,:,3][rPerp_mask] = X[:,:,3][rPerp_mask]/3630.\n",
    "\n",
    "    isnan = np.isnan(X[:,:,0])\n",
    "    anytruth = np.any(isnan)\n",
    "    print(anytruth)\n",
    "\n",
    "    ## Energy Values that are not zero!\n",
    "    E_nonZeroMask = X[:,:,0] != 0\n",
    "    X[:,:,0][E_nonZeroMask] = np.log(X[:,:,0][E_nonZeroMask])\n",
    "    cellE_mean = np.mean(X[:,:,0][E_nonZeroMask])\n",
    "    cellE_std = np.std(X[:,:,0][E_nonZeroMask])\n",
    "    X[:,:,0][E_nonZeroMask] = (X[:,:,0][E_nonZeroMask] - cellE_mean)/cellE_std\n",
    "\n",
    "    ## Eta and Phi\n",
    "    # do nothing for now as a control and check performance\n",
    "    eta_mask = X[:,:,1] != 0\n",
    "    X[:,:,1][eta_mask] = X[:,:,1][eta_mask]/.7\n",
    "\n",
    "    phi_mask = X[:,:,2] != 0\n",
    "    cellPhi_std = np.std(X[:,:,2][phi_mask])\n",
    "    X[:,:,2][phi_mask] = X[:,:,2][phi_mask]/cellPhi_std\n",
    "    t1 = t.time()\n",
    "    print('Time to Normalize: '+str(t1-t0)+' (s)')\n",
    "    return X, Ylog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use for loop to read all the stmc file and write to a single file\n",
    "Nfile = 30\n",
    "fileNames = []\n",
    "startingF = 0\n",
    "for i in range(startingF,startingF+Nfile+1):\n",
    "    fileNames.append(ext_datapath + 'STMC_' + str(i) + '.npz')\n",
    "\n",
    "# create a new h5 file and h5 datasets\n",
    "with h5.File(ext_datapath + 'STMC_train.h5', 'w') as hf:\n",
    "    hf.create_dataset('X', (0,1086,5), maxshape=(None,1086,5), dtype='f')\n",
    "    hf.create_dataset('Y', (0,), maxshape=(None,), dtype='f')\n",
    "\n",
    "# read data from each file, normalized and write to h5 datasets\n",
    "for filename in fileNames:\n",
    "    with h5.File(ext_datapath + 'STMC_train.h5', 'r+') as hf:\n",
    "        with np.load(filename) as data:\n",
    "            data = np.load(filename)\n",
    "            X = data['x']\n",
    "            Y = data['y']\n",
    "            X, Y = normalized_data(X, Y)\n",
    "            hf['X'].resize((hf['X'].shape[0] + X.shape[0]), axis=0)\n",
    "            hf['X'][-X.shape[0]:] = X[:,:,:5]\n",
    "            hf['Y'].resize((hf['Y'].shape[0] + Y.shape[0]), axis=0)\n",
    "            hf['Y'][-Y.shape[0]:] = Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use for loop to read all the stmc file and write to a single file\n",
    "Nfile = 30\n",
    "fileNames = []\n",
    "startingF = 0\n",
    "for i in range(startingF,startingF+Nfile+1):\n",
    "    fileNames.append(ext_datapath + 'STMC_' + str(i) + '.npz')\n",
    "# read data from each file, normalized and write to a single numpy file\n",
    "for filename in fileNames:\n",
    "    with np.load(filename) as data:\n",
    "        X = data['x']\n",
    "        Y = data['y']\n",
    "        X, Y = normalized_data(X, Y)\n",
    "        np.savez(ext_datapath+'STMC_train.npz', x=X, y=Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_npy_file(item):\n",
    "    data = np.load(item.decode())\n",
    "    return data.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nfile = 30\n",
    "fileNames = []\n",
    "startingF = 0\n",
    "for i in range(startingF,startingF+Nfile+1):\n",
    "    fileNames.append(ext_datapath + 'STMC_' + str(i) + '.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices(fileNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(\n",
    "        lambda item: tuple(tf.py_function(read_npy_file, [item], [tf.float32,])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TakeDataset shapes: (<unknown>,), types: (tf.float32,)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b5d44aea0cc164fd0ebfc1732b90238ce53cef8d0e631d031394e4c6aaa004a9"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('tensorflow-gpu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
