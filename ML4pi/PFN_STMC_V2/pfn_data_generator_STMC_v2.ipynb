{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PFN and PFN with mdn layer\n",
    "this note book is only for model training, see result comparison in evaluation notebook\n",
    "\n",
    "STMC_V2 dataset\n",
    "\n",
    "Eta ---->2.5\n",
    "DeltaR ---->1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import time as t\n",
    "import scipy.constants as spc\n",
    "import matplotlib.ticker as ticker\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_prefix = 'D:/Work/EPE/ML4pi/'\n",
    "plotpath = path_prefix+'plots/'\n",
    "modelpath_c = path_prefix+''\n",
    "modelpath = path_prefix+''\n",
    "data_path = path_prefix + \"v7/\"\n",
    "ext_path = \"H:/EPE_file_storage/\"\n",
    "ext_modelpath = ext_path + \"Model/\"\n",
    "ext_datapath = 'H:/EPE_file_storage/data_storage/pipm/root/'\n",
    "stmc_v2_path = ext_datapath + 'STMC_v2/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(path_prefix)\n",
    "from util import resolution_util as ru\n",
    "from util import plot_util as pu\n",
    "from util import ml_util as mu\n",
    "import uproot3 as ur\n",
    "import keras.utils.io_utils as kiu\n",
    "import h5py as h5\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training data\n",
    "with h5.File(stmc_v2_path+'STMC_v2_train.h5', 'r') as hf:\n",
    "    data = hf['X'][:]\n",
    "    target = hf['Y'][:]\n",
    "\n",
    "# load validation data\n",
    "with h5.File(stmc_v2_path+'STMC_v2_val.h5', 'r') as hf:\n",
    "    val_data = hf['X'][:]\n",
    "    val_target = hf['Y'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[:,:,:5]\n",
    "val_data = val_data[:,:,:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(288046, 1389, 5)\n",
      "(288046,)\n",
      "(61724, 1389, 5)\n",
      "(61724,)\n"
     ]
    }
   ],
   "source": [
    "target = target[:,0]\n",
    "val_target = val_target[:,0]\n",
    "print(data.shape)\n",
    "print(target.shape)\n",
    "print(val_data.shape)\n",
    "print(val_target.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use data generator feed input data to avoid memory issue, any one of the three here should work, carefully choose the generator batch sizes would improve the gpu usage rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minibatches(inputs=None, targets=None, batch_size=None, shuffle=True):\n",
    "    while True:\n",
    "        assert len(inputs) == len(targets)\n",
    "        if shuffle:\n",
    "            indices = np.arange(len(inputs))\n",
    "            np.random.shuffle(indices)\n",
    "        for start_idx in range( len(inputs) - batch_size ):\n",
    "            if shuffle:\n",
    "                excerpt = indices[start_idx:start_idx + batch_size]\n",
    "            else:\n",
    "                excerpt = slice(start_idx, start_idx + batch_size)\n",
    "            yield inputs[excerpt], targets[excerpt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(data, targets, batch_size):\n",
    "     batches = (len(data) + batch_size - 1)//batch_size\n",
    "     while True:\n",
    "          for i in range(batches):\n",
    "               X = data[i*batch_size : (i+1)*batch_size]\n",
    "               Y = targets[i*batch_size : (i+1)*batch_size]\n",
    "               yield (X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ML Model setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "from tensorflow.keras.models import load_model\n",
    "import keras.backend as Kb\n",
    "import energyflow as ef\n",
    "from energyflow.archs import PFN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# network architecture parameters\n",
    "Phi_sizes, F_sizes = (100, 100, 128), (100, 100, 100)\n",
    "output_act, output_dim = 'linear', 1\n",
    "loss = 'mse'\n",
    "\n",
    "# network training parameters\n",
    "num_epoch = 1500\n",
    "batch_size = 1000\n",
    "\n",
    "netOpt = tf.keras.optimizers.Adam(\n",
    "    learning_rate=.002,\n",
    "    beta_1=0.9,\n",
    "    beta_2=0.999,\n",
    "    epsilon=1e-09,\n",
    "    amsgrad=False)\n",
    "\n",
    "RMS_prop = tf.keras.optimizers.RMSprop(learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input (InputLayer)              [(None, None, 5)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tdist_0 (TimeDistributed)       (None, None, 100)    600         input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, None, 100)    0           tdist_0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tdist_1 (TimeDistributed)       (None, None, 100)    10100       activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, None, 100)    0           tdist_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tdist_2 (TimeDistributed)       (None, None, 128)    12928       activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "mask (Lambda)                   (None, None)         0           input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, None, 128)    0           tdist_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sum (Dot)                       (None, 128)          0           mask[0][0]                       \n",
      "                                                                 activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_0 (Dense)                 (None, 100)          12900       sum[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 100)          0           dense_0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 100)          10100       activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 100)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 100)          10100       activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 100)          0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "output (Dense)                  (None, 1)            101         activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 1)            0           output[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 56,829\n",
      "Trainable params: 56,829\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# network architecture parameters\n",
    "Phi_sizes, F_sizes = (100, 100, 128), (100, 100, 100)\n",
    "output_act, output_dim = 'linear', 1\n",
    "loss = 'mse'\n",
    "\n",
    "Kb.clear_session()\n",
    "\n",
    "pfn = PFN(input_dim=5, Phi_sizes=Phi_sizes, F_sizes=F_sizes, \n",
    "          output_act=output_act, output_dim=output_dim, loss=loss,\n",
    "          optimizer=netOpt, metrics=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "281/281 [==============================] - 25s 87ms/step - loss: 0.0109 - val_loss: 0.0296\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.02965, saving model to model\\pfn_stmc_v2_dg.h5\n",
      "Epoch 2/100\n",
      "281/281 [==============================] - 25s 88ms/step - loss: 0.0085 - val_loss: 0.0250\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.02965 to 0.02502, saving model to model\\pfn_stmc_v2_dg.h5\n",
      "Epoch 3/100\n",
      "281/281 [==============================] - 25s 88ms/step - loss: 0.0102 - val_loss: 0.0278\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.02502\n",
      "Epoch 4/100\n",
      "281/281 [==============================] - 25s 87ms/step - loss: 0.0083 - val_loss: 0.0251\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.02502\n",
      "Epoch 5/100\n",
      "281/281 [==============================] - 25s 89ms/step - loss: 0.0106 - val_loss: 0.1106\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.02502\n",
      "Epoch 6/100\n",
      "281/281 [==============================] - 26s 92ms/step - loss: 0.0117 - val_loss: 0.0367\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.02502\n",
      "Epoch 7/100\n",
      "281/281 [==============================] - 26s 92ms/step - loss: 0.0097 - val_loss: 0.0414\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.02502\n",
      "Epoch 8/100\n",
      "281/281 [==============================] - 26s 93ms/step - loss: 0.0087 - val_loss: 0.0506\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.02502\n",
      "Epoch 9/100\n",
      "281/281 [==============================] - 26s 93ms/step - loss: 0.0081 - val_loss: 0.0434\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.02502\n",
      "Epoch 10/100\n",
      "281/281 [==============================] - 30s 106ms/step - loss: 0.0072 - val_loss: 0.0388\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.02502\n",
      "Epoch 11/100\n",
      "281/281 [==============================] - 26s 93ms/step - loss: 0.0082 - val_loss: 0.0391\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.02502\n",
      "Epoch 12/100\n",
      "281/281 [==============================] - 26s 94ms/step - loss: 0.0065 - val_loss: 0.0339\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.02502\n",
      "Epoch 13/100\n",
      "281/281 [==============================] - 27s 95ms/step - loss: 0.0070 - val_loss: 0.0284\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.02502\n",
      "Epoch 14/100\n",
      "281/281 [==============================] - 27s 96ms/step - loss: 0.0068 - val_loss: 0.0257\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.02502\n",
      "Epoch 15/100\n",
      "281/281 [==============================] - 27s 96ms/step - loss: 0.0062 - val_loss: 0.0273\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.02502\n",
      "Epoch 16/100\n",
      "281/281 [==============================] - 27s 95ms/step - loss: 0.0056 - val_loss: 0.0285\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.02502\n",
      "Epoch 17/100\n",
      "281/281 [==============================] - 26s 94ms/step - loss: 0.0051 - val_loss: 0.0330\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.02502\n",
      "Epoch 18/100\n",
      "281/281 [==============================] - 26s 94ms/step - loss: 0.0051 - val_loss: 0.0316\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.02502\n",
      "Epoch 19/100\n",
      "281/281 [==============================] - 26s 94ms/step - loss: 0.0052 - val_loss: 0.0273\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.02502\n",
      "Epoch 20/100\n",
      "281/281 [==============================] - 26s 94ms/step - loss: 0.0054 - val_loss: 0.0274\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.02502\n",
      "Epoch 21/100\n",
      "281/281 [==============================] - 26s 94ms/step - loss: 0.0054 - val_loss: 0.0241\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.02502 to 0.02415, saving model to model\\pfn_stmc_v2_dg.h5\n",
      "Epoch 22/100\n",
      "281/281 [==============================] - 27s 95ms/step - loss: 0.0050 - val_loss: 0.0208\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.02415 to 0.02075, saving model to model\\pfn_stmc_v2_dg.h5\n",
      "Epoch 23/100\n",
      "281/281 [==============================] - 27s 94ms/step - loss: 0.0041 - val_loss: 0.0225\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.02075\n",
      "Epoch 24/100\n",
      "281/281 [==============================] - 27s 95ms/step - loss: 0.0040 - val_loss: 0.0205\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.02075 to 0.02051, saving model to model\\pfn_stmc_v2_dg.h5\n",
      "Epoch 25/100\n",
      "281/281 [==============================] - 27s 95ms/step - loss: 0.0041 - val_loss: 0.0207\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.02051\n",
      "Epoch 26/100\n",
      "281/281 [==============================] - 27s 94ms/step - loss: 0.0040 - val_loss: 0.0207\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.02051\n",
      "Epoch 27/100\n",
      "281/281 [==============================] - 27s 94ms/step - loss: 0.0037 - val_loss: 0.0189\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.02051 to 0.01894, saving model to model\\pfn_stmc_v2_dg.h5\n",
      "Epoch 28/100\n",
      "281/281 [==============================] - 27s 95ms/step - loss: 0.0045 - val_loss: 0.0196\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.01894\n",
      "Epoch 29/100\n",
      "281/281 [==============================] - 26s 94ms/step - loss: 0.0041 - val_loss: 0.0248\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.01894\n",
      "Epoch 30/100\n",
      "281/281 [==============================] - 24s 86ms/step - loss: 0.0038 - val_loss: 0.0217\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.01894\n",
      "Epoch 31/100\n",
      "281/281 [==============================] - 25s 89ms/step - loss: 0.0037 - val_loss: 0.0305\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.01894\n",
      "Epoch 32/100\n",
      "281/281 [==============================] - 24s 86ms/step - loss: 0.0049 - val_loss: 0.0560\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.01894\n",
      "Epoch 33/100\n",
      "281/281 [==============================] - 24s 86ms/step - loss: 0.0055 - val_loss: 0.0291\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.01894\n",
      "Epoch 34/100\n",
      "281/281 [==============================] - 24s 86ms/step - loss: 0.0053 - val_loss: 0.0302\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.01894\n",
      "Epoch 35/100\n",
      "281/281 [==============================] - 24s 86ms/step - loss: 0.0041 - val_loss: 0.0286\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.01894\n",
      "Epoch 36/100\n",
      "281/281 [==============================] - 24s 86ms/step - loss: 0.0160 - val_loss: 0.0281\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.01894\n",
      "Epoch 37/100\n",
      "281/281 [==============================] - 24s 87ms/step - loss: 0.0071 - val_loss: 0.2140\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.01894\n",
      "Epoch 38/100\n",
      "281/281 [==============================] - 24s 87ms/step - loss: 0.0048 - val_loss: 0.1228\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.01894\n",
      "Epoch 39/100\n",
      "281/281 [==============================] - 24s 87ms/step - loss: 0.0051 - val_loss: 0.0939\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.01894\n",
      "Epoch 40/100\n",
      "281/281 [==============================] - 24s 87ms/step - loss: 0.0043 - val_loss: 0.0340\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.01894\n",
      "Epoch 41/100\n",
      "281/281 [==============================] - 24s 87ms/step - loss: 0.0040 - val_loss: 0.0226\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.01894\n",
      "Epoch 42/100\n",
      "281/281 [==============================] - 25s 88ms/step - loss: 0.0045 - val_loss: 0.0178\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.01894 to 0.01784, saving model to model\\pfn_stmc_v2_dg.h5\n",
      "Epoch 43/100\n",
      "281/281 [==============================] - 24s 87ms/step - loss: 0.0038 - val_loss: 0.0185\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.01784\n",
      "Epoch 44/100\n",
      "281/281 [==============================] - 24s 86ms/step - loss: 0.0036 - val_loss: 0.0202\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.01784\n",
      "Epoch 45/100\n",
      "281/281 [==============================] - 24s 86ms/step - loss: 0.0037 - val_loss: 0.0180\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.01784\n",
      "Epoch 46/100\n",
      "281/281 [==============================] - 24s 86ms/step - loss: 0.0048 - val_loss: 0.0167\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.01784 to 0.01674, saving model to model\\pfn_stmc_v2_dg.h5\n",
      "Epoch 47/100\n",
      "281/281 [==============================] - 24s 86ms/step - loss: 0.0037 - val_loss: 0.0179\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.01674\n",
      "Epoch 48/100\n",
      "281/281 [==============================] - 24s 87ms/step - loss: 0.0982 - val_loss: 0.0519\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.01674\n",
      "Epoch 49/100\n",
      "281/281 [==============================] - 24s 87ms/step - loss: 0.0180 - val_loss: 0.0548\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.01674\n",
      "Epoch 50/100\n",
      "281/281 [==============================] - 24s 86ms/step - loss: 0.0142 - val_loss: 0.0729\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.01674\n",
      "Epoch 51/100\n",
      "281/281 [==============================] - 24s 86ms/step - loss: 0.0100 - val_loss: 0.0731\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.01674\n",
      "Epoch 52/100\n",
      "281/281 [==============================] - 24s 86ms/step - loss: 0.0095 - val_loss: 0.0704\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.01674\n",
      "Epoch 53/100\n",
      "281/281 [==============================] - 24s 86ms/step - loss: 0.0081 - val_loss: 0.0686\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.01674\n",
      "Epoch 54/100\n",
      "281/281 [==============================] - 24s 86ms/step - loss: 0.0078 - val_loss: 0.0762\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.01674\n",
      "Epoch 55/100\n",
      "281/281 [==============================] - 24s 86ms/step - loss: 0.0087 - val_loss: 0.0739\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.01674\n",
      "Epoch 56/100\n",
      "281/281 [==============================] - 24s 86ms/step - loss: 0.0086 - val_loss: 0.0692\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.01674\n",
      "Epoch 57/100\n",
      "281/281 [==============================] - 24s 86ms/step - loss: 0.0083 - val_loss: 0.0680\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.01674\n",
      "Epoch 58/100\n",
      "281/281 [==============================] - 24s 86ms/step - loss: 0.0074 - val_loss: 0.0693\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.01674\n",
      "Epoch 59/100\n",
      "281/281 [==============================] - 24s 87ms/step - loss: 0.0094 - val_loss: 0.0695\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.01674\n",
      "Epoch 60/100\n",
      "281/281 [==============================] - 24s 87ms/step - loss: 0.0084 - val_loss: 0.0668\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.01674\n",
      "Epoch 61/100\n",
      "281/281 [==============================] - 24s 86ms/step - loss: 0.0062 - val_loss: 0.0655\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.01674\n",
      "Epoch 62/100\n",
      "281/281 [==============================] - 24s 86ms/step - loss: 0.0061 - val_loss: 0.0752\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.01674\n",
      "Epoch 63/100\n",
      "281/281 [==============================] - 24s 86ms/step - loss: 0.0063 - val_loss: 0.0724\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.01674\n",
      "Epoch 64/100\n",
      "281/281 [==============================] - 24s 87ms/step - loss: 0.0043 - val_loss: 0.0711\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.01674\n",
      "Epoch 65/100\n",
      "281/281 [==============================] - 24s 87ms/step - loss: 0.0042 - val_loss: 0.0674\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.01674\n",
      "Epoch 66/100\n",
      "281/281 [==============================] - 25s 87ms/step - loss: 0.0044 - val_loss: 0.0331\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.01674\n",
      "Epoch 67/100\n",
      "281/281 [==============================] - 25s 88ms/step - loss: 0.0056 - val_loss: 0.0257\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.01674\n",
      "Epoch 68/100\n",
      "281/281 [==============================] - 24s 86ms/step - loss: 0.0056 - val_loss: 0.0367\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.01674\n",
      "Epoch 69/100\n",
      "281/281 [==============================] - 24s 86ms/step - loss: 0.0087 - val_loss: 0.0414\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.01674\n",
      "Epoch 70/100\n",
      "281/281 [==============================] - 24s 86ms/step - loss: 0.0102 - val_loss: 0.0401\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.01674\n",
      "Epoch 71/100\n",
      "281/281 [==============================] - 73s 261ms/step - loss: 0.0111 - val_loss: 0.0422\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.01674\n",
      "Epoch 72/100\n",
      "281/281 [==============================] - 84s 297ms/step - loss: 0.0049 - val_loss: 0.0460\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.01674\n",
      "Epoch 73/100\n",
      "281/281 [==============================] - 83s 295ms/step - loss: 0.0077 - val_loss: 0.0746\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.01674\n",
      "Epoch 74/100\n",
      "281/281 [==============================] - 71s 252ms/step - loss: 0.0061 - val_loss: 0.0749\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.01674\n",
      "Epoch 75/100\n",
      "281/281 [==============================] - 107s 380ms/step - loss: 0.0049 - val_loss: 0.0673\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.01674\n",
      "Epoch 76/100\n",
      "281/281 [==============================] - 86s 307ms/step - loss: 0.0052 - val_loss: 0.0546\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.01674\n",
      "Epoch 77/100\n",
      "281/281 [==============================] - 88s 315ms/step - loss: 0.0040 - val_loss: 0.0518\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.01674\n",
      "Epoch 78/100\n",
      "281/281 [==============================] - 89s 316ms/step - loss: 0.0033 - val_loss: 0.0461\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.01674\n",
      "Epoch 79/100\n",
      "281/281 [==============================] - 88s 314ms/step - loss: 0.0035 - val_loss: 0.0422\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.01674\n",
      "Epoch 80/100\n",
      "281/281 [==============================] - 89s 315ms/step - loss: 0.0036 - val_loss: 0.0418\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.01674\n",
      "Epoch 81/100\n",
      "281/281 [==============================] - 88s 312ms/step - loss: 0.0041 - val_loss: 0.0382\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.01674\n",
      "Epoch 82/100\n",
      "281/281 [==============================] - 90s 321ms/step - loss: 0.0037 - val_loss: 0.0428\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.01674\n",
      "Epoch 83/100\n",
      "281/281 [==============================] - 87s 310ms/step - loss: 0.0039 - val_loss: 0.0417\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.01674\n",
      "Epoch 84/100\n",
      "281/281 [==============================] - 82s 291ms/step - loss: 0.0039 - val_loss: 0.0346\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.01674\n",
      "Epoch 85/100\n",
      "281/281 [==============================] - 83s 296ms/step - loss: 0.0045 - val_loss: 0.0432\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.01674\n",
      "Epoch 86/100\n",
      "281/281 [==============================] - 80s 285ms/step - loss: 0.0053 - val_loss: 0.0542\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.01674\n",
      "Epoch 87/100\n",
      "281/281 [==============================] - 48s 171ms/step - loss: 0.0044 - val_loss: 0.0503\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.01674\n",
      "Epoch 88/100\n",
      "281/281 [==============================] - 74s 264ms/step - loss: 0.0039 - val_loss: 0.0452\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.01674\n",
      "Epoch 89/100\n",
      "281/281 [==============================] - 91s 325ms/step - loss: 0.0045 - val_loss: 0.0617\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.01674\n",
      "Epoch 90/100\n",
      "281/281 [==============================] - 90s 322ms/step - loss: 0.0042 - val_loss: 0.0363\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.01674\n",
      "Epoch 91/100\n",
      "281/281 [==============================] - 80s 284ms/step - loss: 0.0036 - val_loss: 0.0305\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.01674\n",
      "Epoch 92/100\n",
      "281/281 [==============================] - 100s 357ms/step - loss: 0.0041 - val_loss: 0.0270\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.01674\n",
      "Epoch 93/100\n",
      "281/281 [==============================] - 93s 333ms/step - loss: 0.0047 - val_loss: 0.0240\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.01674\n",
      "Epoch 94/100\n",
      "281/281 [==============================] - 85s 302ms/step - loss: 0.0060 - val_loss: 0.0933\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.01674\n",
      "Epoch 95/100\n",
      "281/281 [==============================] - 87s 311ms/step - loss: 0.0031 - val_loss: 0.0354\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.01674\n",
      "Epoch 96/100\n",
      "281/281 [==============================] - 89s 317ms/step - loss: 0.0033 - val_loss: 0.0490\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.01674\n",
      "Epoch 97/100\n",
      "281/281 [==============================] - 93s 329ms/step - loss: 0.0040 - val_loss: 0.2128\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.01674\n",
      "Epoch 98/100\n",
      "281/281 [==============================] - 107s 379ms/step - loss: 0.0049 - val_loss: 0.0314\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.01674\n",
      "Epoch 99/100\n",
      "281/281 [==============================] - 106s 376ms/step - loss: 0.0031 - val_loss: 0.0616\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.01674\n",
      "Epoch 100/100\n",
      "281/281 [==============================] - 106s 378ms/step - loss: 0.0036 - val_loss: 0.0441\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.01674\n",
      "Time to train: 4377.288618803024 (s)\n"
     ]
    }
   ],
   "source": [
    "model_name = 'pfn_stmc_v2_dg'\n",
    "chkpoint = tf.keras.callbacks.ModelCheckpoint('model/' + model_name + '.h5', monitor='val_loss', verbose=1, save_best_only=True)\n",
    "history_logger = tf.keras.callbacks.CSVLogger('log/' + model_name + '.log', separator=',', append=True)\n",
    "t0 = t.time()\n",
    "\"\"\"history_ct = pfn.fit(X_train, Y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=500,\n",
    "                    validation_data=(X_val, Y_val),\n",
    "                    callbacks=[chkpoint, history_logger])\"\"\"\n",
    "if os.path.isfile('model/' + model_name + '.h5'):\n",
    "    pfn.load_weights('model/' + model_name + '.h5')\n",
    "\n",
    "history_ct = pfn.fit(minibatches(data, target, batch_size=1024),\n",
    "                    steps_per_epoch=len(data)//1024,\n",
    "                    validation_data=(minibatches(val_data, val_target, batch_size=1024)),\n",
    "                    validation_steps=len(val_data)//1024,\n",
    "                    epochs=100,\n",
    "                    verbose=1,\n",
    "                    callbacks=[chkpoint, history_logger])\n",
    "t1 = t.time()\n",
    "print('Time to train: '+str(t1-t0)+' (s)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "tf.config.experimental.set_memory_growth(physical_gpus[0],True)\n",
    "logical_gpus = tf.config.list_logical_devices(\"GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6,6))\n",
    "plt.plot(history_ct.history['val_loss'], label=\"Validation\",linestyle='dashed')\n",
    "plt.plot(history_ct.history['loss'], label=\"Training\")\n",
    "plt.yscale('log')\n",
    "plt.ylim(.001,10)\n",
    "plt.yticks(fontsize=13)\n",
    "plt.xlim(0,1000)\n",
    "plt.xticks(fontsize=13)\n",
    "plt.xlabel('Epochs', fontsize=14)\n",
    "plt.ylabel('Loss', fontsize=14)\n",
    "plt.legend(loc='upper right', ncol=1)\n",
    "plt.text(1000, 1.5, 'LR=2e-3', fontsize=13)\n",
    "plt.text(1000, 1, 'Epoch: 1000', fontsize=13)\n",
    "# plt.tight_layout()\n",
    "# plt.savefig('Regression_Plots/July/XY_STSC_lossCurves_3000batch_LR1e-2_2021-07-016.png', format='png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "# from . import plot_util as pu\n",
    "from util import plot_util as pu\n",
    "\n",
    "def _iqrOverMed(x):\n",
    "    # get the IQR via the percentile function\n",
    "    # 84 is median + 1 sigma, 16 is median - 1 sigma\n",
    "    q16, q84 = np.percentile(x, [16, 84])\n",
    "    return (q84 - q16) / (2 * np.median(x))\n",
    "\n",
    "def resolutionPlot(x, y, figfile='',\n",
    "                   xlabel='True Energy [GeV]', ylabel='Response IQR / (2 x Median)',\n",
    "                   atlas_x=-1, atlas_y=-1, simulation=False,\n",
    "                   xlim=(0.3,1000), ylim=(0,1), \n",
    "                   textlist=[]):\n",
    "    xbin = [10**exp for exp in  np.arange(-1.0, 3.1, 0.1)]\n",
    "    xcenter = [(xbin[i] + xbin[i+1]) / 2 for i in range(len(xbin)-1)]\n",
    "\n",
    "    resolution = stats.binned_statistic(x, y, bins=xbin, statistic=_iqrOverMed).statistic\n",
    "    \n",
    "    plt.cla(); plt.clf()\n",
    "    fig = plt.figure()\n",
    "    fig.patch.set_facecolor('white')\n",
    "    plt.plot(xcenter, resolution)\n",
    "    plt.xscale('log')\n",
    "    plt.xlim(xlim)\n",
    "    plt.ylim(ylim)\n",
    "    pu.ampl.set_xlabel(xlabel)\n",
    "    pu.ampl.set_ylabel(ylabel)\n",
    "\n",
    "    pu.drawLabels(fig, atlas_x, atlas_y, simulation, textlist)\n",
    "\n",
    "    if figfile != '':\n",
    "        plt.savefig(figfile)\n",
    "    plt.show()\n",
    "\n",
    "    return xcenter, resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_probability as tfp\n",
    "\n",
    "def point_mask_fn(X, mask_val=0.):\n",
    "    return Kb.cast(Kb.any(Kb.not_equal(X, mask_val), axis=-1), Kb.dtype(X))\n",
    "\n",
    "def convert_to_tensor(X):\n",
    "    return tf.concat([tfp.distributions.Distribution.mean(X), tfp.distributions.Distribution.stddev(X)],1)\n",
    "\n",
    "def ParticleFlow_MDN(num_features, name=\"PFN_MDN_Network\"):\n",
    "    \n",
    "    event_shape = [1]\n",
    "    num_components = 3\n",
    "    params_size = tfp.layers.MixtureNormal.params_size(num_components, event_shape)\n",
    "\n",
    "\n",
    "    inputs = keras.Input(shape=(None, num_features), name='input')\n",
    "\n",
    "    dense_0 = layers.Dense(100)\n",
    "    t_dist_0 = layers.TimeDistributed(dense_0, name='t_dist_0')(inputs)\n",
    "    activation_0 = layers.Activation('relu', name=\"activation_0\")(t_dist_0)\n",
    "    \n",
    "    dense_1 = layers.Dense(100)\n",
    "    t_dist_1 = layers.TimeDistributed(dense_1, name='t_dist_1')(activation_0)\n",
    "    activation_1 = layers.Activation('relu', name='activation_1')(t_dist_1)\n",
    "    \n",
    "    dense_2 = layers.Dense(128)\n",
    "    t_dist_2 = layers.TimeDistributed(dense_2, name='t_dist_2')(activation_1)\n",
    "    activation_2 = layers.Activation('relu', name='activation_2')(t_dist_2)\n",
    "    \n",
    "    lambda_layer = layers.Lambda(point_mask_fn, output_shape=(None, None),\n",
    "                                mask=None,\n",
    "                                name='mask')(inputs)\n",
    "\n",
    "    sum_layer = layers.Dot(axes=(1,1), name='sum')([lambda_layer, activation_2])\n",
    "    \n",
    "    dense_3 = layers.Dense(100, name='dense_0')(sum_layer)\n",
    "    activation_3 = layers.Activation('relu', name=\"activation_3\")(dense_3)\n",
    "    \n",
    "    dense_4 = layers.Dense(100, name='dense_1')(activation_3)\n",
    "    activation_4 = layers.Activation('relu', name=\"activation_4\")(dense_4)\n",
    "    \n",
    "    dense_5 = layers.Dense(100, name='dense_2')(activation_4)\n",
    "    activation_5 = layers.Activation('relu', name=\"activation_5\")(dense_5)\n",
    "    \n",
    "    dense_6 = layers.Dense(units=params_size, activation=lambda x: tf.clip_by_value(x, -30., 30.))(activation_5)\n",
    "\n",
    "    \n",
    "    mdn_0 = tfp.layers.MixtureNormal(num_components, event_shape, validate_args=True,\n",
    "                                          convert_to_tensor_fn=convert_to_tensor)(dense_6)\n",
    "    \n",
    "    return keras.Model(inputs=inputs, outputs=mdn_0, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"PFN_MDN_Network\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input (InputLayer)              [(None, None, 5)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "t_dist_0 (TimeDistributed)      (None, None, 100)    600         input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_0 (Activation)       (None, None, 100)    0           t_dist_0[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "t_dist_1 (TimeDistributed)      (None, None, 100)    10100       activation_0[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, None, 100)    0           t_dist_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "t_dist_2 (TimeDistributed)      (None, None, 128)    12928       activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "mask (Lambda)                   (None, None)         0           input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, None, 128)    0           t_dist_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "sum (Dot)                       (None, 128)          0           mask[0][0]                       \n",
      "                                                                 activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_0 (Dense)                 (None, 100)          12900       sum[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 100)          0           dense_0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 100)          10100       activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 100)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 100)          10100       activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 100)          0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 9)            909         activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "mixture_normal (MixtureNormal)  multiple             0           dense_3[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 57,637\n",
      "Trainable params: 57,637\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "Kb.clear_session()\n",
    "\n",
    "netOpt = tf.keras.optimizers.Adam(\n",
    "    learning_rate=.003,\n",
    "    beta_1=0.9,\n",
    "    beta_2=0.999,\n",
    "    epsilon=1e-09,\n",
    "    amsgrad=False)\n",
    "\n",
    "PFN_mdn = ParticleFlow_MDN(num_features=5)\n",
    "PFN_mdn.compile(optimizer = netOpt, loss=lambda y, p_y: -p_y.log_prob(y))\n",
    "PFN_mdn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "281/281 [==============================] - 26s 91ms/step - loss: 1.8519 - val_loss: 0.4893\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.48933, saving model to model\\PFN_MDN_STMC_v2.h5\n",
      "Epoch 2/100\n",
      "281/281 [==============================] - 25s 91ms/step - loss: 0.4258 - val_loss: -0.0701\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.48933 to -0.07008, saving model to model\\PFN_MDN_STMC_v2.h5\n",
      "Epoch 3/100\n",
      "281/281 [==============================] - 26s 93ms/step - loss: 0.2150 - val_loss: -0.1012\n",
      "\n",
      "Epoch 00003: val_loss improved from -0.07008 to -0.10115, saving model to model\\PFN_MDN_STMC_v2.h5\n",
      "Epoch 4/100\n",
      "281/281 [==============================] - 26s 91ms/step - loss: -0.1976 - val_loss: -0.3705\n",
      "\n",
      "Epoch 00004: val_loss improved from -0.10115 to -0.37049, saving model to model\\PFN_MDN_STMC_v2.h5\n",
      "Epoch 5/100\n",
      "281/281 [==============================] - 43s 152ms/step - loss: -0.5833 - val_loss: 1.0339\n",
      "\n",
      "Epoch 00005: val_loss did not improve from -0.37049\n",
      "Epoch 6/100\n",
      "281/281 [==============================] - 27s 97ms/step - loss: 0.0613 - val_loss: -0.8424\n",
      "\n",
      "Epoch 00006: val_loss improved from -0.37049 to -0.84245, saving model to model\\PFN_MDN_STMC_v2.h5\n",
      "Epoch 7/100\n",
      "281/281 [==============================] - 27s 95ms/step - loss: -0.7768 - val_loss: -0.7844\n",
      "\n",
      "Epoch 00007: val_loss did not improve from -0.84245\n",
      "Epoch 8/100\n",
      "281/281 [==============================] - 27s 94ms/step - loss: -0.9484 - val_loss: -0.9370\n",
      "\n",
      "Epoch 00008: val_loss improved from -0.84245 to -0.93702, saving model to model\\PFN_MDN_STMC_v2.h5\n",
      "Epoch 9/100\n",
      "281/281 [==============================] - 29s 103ms/step - loss: -1.1063 - val_loss: -0.9623\n",
      "\n",
      "Epoch 00009: val_loss improved from -0.93702 to -0.96228, saving model to model\\PFN_MDN_STMC_v2.h5\n",
      "Epoch 10/100\n",
      "281/281 [==============================] - 35s 126ms/step - loss: -0.7775 - val_loss: -0.3747\n",
      "\n",
      "Epoch 00010: val_loss did not improve from -0.96228\n",
      "Epoch 11/100\n",
      "281/281 [==============================] - 23s 83ms/step - loss: -1.1444 - val_loss: -1.1212\n",
      "\n",
      "Epoch 00011: val_loss improved from -0.96228 to -1.12124, saving model to model\\PFN_MDN_STMC_v2.h5\n",
      "Epoch 12/100\n",
      "281/281 [==============================] - 23s 83ms/step - loss: -1.3893 - val_loss: -1.1908\n",
      "\n",
      "Epoch 00012: val_loss improved from -1.12124 to -1.19076, saving model to model\\PFN_MDN_STMC_v2.h5\n",
      "Epoch 13/100\n",
      "281/281 [==============================] - 23s 83ms/step - loss: -1.3728 - val_loss: -1.2509\n",
      "\n",
      "Epoch 00013: val_loss improved from -1.19076 to -1.25090, saving model to model\\PFN_MDN_STMC_v2.h5\n",
      "Epoch 14/100\n",
      "281/281 [==============================] - 24s 84ms/step - loss: -1.4130 - val_loss: -0.4933\n",
      "\n",
      "Epoch 00014: val_loss did not improve from -1.25090\n",
      "Epoch 15/100\n",
      "281/281 [==============================] - 24s 85ms/step - loss: -1.4271 - val_loss: -0.9509\n",
      "\n",
      "Epoch 00015: val_loss did not improve from -1.25090\n",
      "Epoch 16/100\n",
      "281/281 [==============================] - 25s 90ms/step - loss: -1.4080 - val_loss: -1.0480\n",
      "\n",
      "Epoch 00016: val_loss did not improve from -1.25090\n",
      "Epoch 17/100\n",
      "281/281 [==============================] - 25s 88ms/step - loss: -1.3146 - val_loss: -1.2586\n",
      "\n",
      "Epoch 00017: val_loss improved from -1.25090 to -1.25857, saving model to model\\PFN_MDN_STMC_v2.h5\n",
      "Epoch 18/100\n",
      "281/281 [==============================] - 25s 88ms/step - loss: -1.4215 - val_loss: -1.2555\n",
      "\n",
      "Epoch 00018: val_loss did not improve from -1.25857\n",
      "Epoch 19/100\n",
      "281/281 [==============================] - 24s 86ms/step - loss: -1.4423 - val_loss: -0.8597\n",
      "\n",
      "Epoch 00019: val_loss did not improve from -1.25857\n",
      "Epoch 20/100\n",
      "281/281 [==============================] - 24s 87ms/step - loss: -1.3683 - val_loss: -0.9991\n",
      "\n",
      "Epoch 00020: val_loss did not improve from -1.25857\n",
      "Epoch 21/100\n",
      "281/281 [==============================] - 36s 130ms/step - loss: -1.4711 - val_loss: -1.0280\n",
      "\n",
      "Epoch 00021: val_loss did not improve from -1.25857\n",
      "Epoch 22/100\n",
      "281/281 [==============================] - 25s 89ms/step - loss: -1.4425 - val_loss: -0.4087\n",
      "\n",
      "Epoch 00022: val_loss did not improve from -1.25857\n",
      "Epoch 23/100\n",
      "281/281 [==============================] - 25s 90ms/step - loss: -1.4342 - val_loss: -1.1162\n",
      "\n",
      "Epoch 00023: val_loss did not improve from -1.25857\n",
      "Epoch 24/100\n",
      "281/281 [==============================] - 25s 90ms/step - loss: -1.4202 - val_loss: -1.1553\n",
      "\n",
      "Epoch 00024: val_loss did not improve from -1.25857\n",
      "Epoch 25/100\n",
      "281/281 [==============================] - 25s 90ms/step - loss: -1.5533 - val_loss: -1.0957\n",
      "\n",
      "Epoch 00025: val_loss did not improve from -1.25857\n",
      "Epoch 26/100\n",
      "281/281 [==============================] - 25s 90ms/step - loss: -1.5165 - val_loss: -1.2159\n",
      "\n",
      "Epoch 00026: val_loss did not improve from -1.25857\n",
      "Epoch 27/100\n",
      "281/281 [==============================] - 25s 90ms/step - loss: -1.4067 - val_loss: -0.8301\n",
      "\n",
      "Epoch 00027: val_loss did not improve from -1.25857\n",
      "Epoch 28/100\n",
      "281/281 [==============================] - 25s 90ms/step - loss: -1.6173 - val_loss: -1.1177\n",
      "\n",
      "Epoch 00028: val_loss did not improve from -1.25857\n",
      "Epoch 29/100\n",
      "281/281 [==============================] - 25s 90ms/step - loss: -1.6218 - val_loss: -0.9428\n",
      "\n",
      "Epoch 00029: val_loss did not improve from -1.25857\n",
      "Epoch 30/100\n",
      "281/281 [==============================] - 25s 91ms/step - loss: -1.6109 - val_loss: -1.0684\n",
      "\n",
      "Epoch 00030: val_loss did not improve from -1.25857\n",
      "Epoch 31/100\n",
      "281/281 [==============================] - 25s 91ms/step - loss: -1.7338 - val_loss: -0.5606\n",
      "\n",
      "Epoch 00031: val_loss did not improve from -1.25857\n",
      "Epoch 32/100\n",
      "281/281 [==============================] - 25s 90ms/step - loss: -1.5689 - val_loss: -1.2241\n",
      "\n",
      "Epoch 00032: val_loss did not improve from -1.25857\n",
      "Epoch 33/100\n",
      "281/281 [==============================] - 25s 90ms/step - loss: -1.5926 - val_loss: -1.3166\n",
      "\n",
      "Epoch 00033: val_loss improved from -1.25857 to -1.31663, saving model to model\\PFN_MDN_STMC_v2.h5\n",
      "Epoch 34/100\n",
      "281/281 [==============================] - 37s 133ms/step - loss: -1.7124 - val_loss: -1.0998\n",
      "\n",
      "Epoch 00034: val_loss did not improve from -1.31663\n",
      "Epoch 35/100\n",
      "281/281 [==============================] - 25s 91ms/step - loss: -1.7060 - val_loss: -0.7972\n",
      "\n",
      "Epoch 00035: val_loss did not improve from -1.31663\n",
      "Epoch 36/100\n",
      "281/281 [==============================] - 25s 91ms/step - loss: -1.6833 - val_loss: -0.7037\n",
      "\n",
      "Epoch 00036: val_loss did not improve from -1.31663\n",
      "Epoch 37/100\n",
      "281/281 [==============================] - 25s 90ms/step - loss: -1.6816 - val_loss: -0.2838\n",
      "\n",
      "Epoch 00037: val_loss did not improve from -1.31663\n",
      "Epoch 38/100\n",
      "281/281 [==============================] - 25s 91ms/step - loss: -1.7656 - val_loss: -0.2495\n",
      "\n",
      "Epoch 00038: val_loss did not improve from -1.31663\n",
      "Epoch 39/100\n",
      "281/281 [==============================] - 26s 91ms/step - loss: -1.5748 - val_loss: -0.7675\n",
      "\n",
      "Epoch 00039: val_loss did not improve from -1.31663\n",
      "Epoch 40/100\n",
      "281/281 [==============================] - 26s 92ms/step - loss: -1.7011 - val_loss: -1.1135\n",
      "\n",
      "Epoch 00040: val_loss did not improve from -1.31663\n",
      "Epoch 41/100\n",
      "281/281 [==============================] - 26s 92ms/step - loss: -1.7082 - val_loss: -0.7929\n",
      "\n",
      "Epoch 00041: val_loss did not improve from -1.31663\n",
      "Epoch 42/100\n",
      "281/281 [==============================] - 25s 90ms/step - loss: -1.7103 - val_loss: -0.9056\n",
      "\n",
      "Epoch 00042: val_loss did not improve from -1.31663\n",
      "Epoch 43/100\n",
      "281/281 [==============================] - 25s 90ms/step - loss: -1.6980 - val_loss: 0.1752\n",
      "\n",
      "Epoch 00043: val_loss did not improve from -1.31663\n",
      "Epoch 44/100\n",
      "281/281 [==============================] - 25s 90ms/step - loss: -1.4930 - val_loss: -1.1974\n",
      "\n",
      "Epoch 00044: val_loss did not improve from -1.31663\n",
      "Epoch 45/100\n",
      "281/281 [==============================] - 25s 90ms/step - loss: -1.6780 - val_loss: -0.2757\n",
      "\n",
      "Epoch 00045: val_loss did not improve from -1.31663\n",
      "Epoch 46/100\n",
      "281/281 [==============================] - 25s 91ms/step - loss: -1.6857 - val_loss: -0.9513\n",
      "\n",
      "Epoch 00046: val_loss did not improve from -1.31663\n",
      "Epoch 47/100\n",
      "281/281 [==============================] - 25s 90ms/step - loss: -1.7046 - val_loss: -0.7521\n",
      "\n",
      "Epoch 00047: val_loss did not improve from -1.31663\n",
      "Epoch 48/100\n",
      "281/281 [==============================] - 25s 90ms/step - loss: -1.8473 - val_loss: 0.0768\n",
      "\n",
      "Epoch 00048: val_loss did not improve from -1.31663\n",
      "Epoch 49/100\n",
      "281/281 [==============================] - 25s 90ms/step - loss: -1.6481 - val_loss: -0.7710\n",
      "\n",
      "Epoch 00049: val_loss did not improve from -1.31663\n",
      "Epoch 50/100\n",
      "281/281 [==============================] - 25s 90ms/step - loss: -1.7070 - val_loss: 0.3951\n",
      "\n",
      "Epoch 00050: val_loss did not improve from -1.31663\n",
      "Epoch 51/100\n",
      "281/281 [==============================] - 25s 90ms/step - loss: -1.7430 - val_loss: -0.1657\n",
      "\n",
      "Epoch 00051: val_loss did not improve from -1.31663\n",
      "Epoch 52/100\n",
      "281/281 [==============================] - 25s 90ms/step - loss: -1.8316 - val_loss: 0.2069\n",
      "\n",
      "Epoch 00052: val_loss did not improve from -1.31663\n",
      "Epoch 53/100\n",
      "281/281 [==============================] - 25s 90ms/step - loss: -1.7823 - val_loss: 3.5676\n",
      "\n",
      "Epoch 00053: val_loss did not improve from -1.31663\n",
      "Epoch 54/100\n",
      "281/281 [==============================] - 25s 90ms/step - loss: -1.8027 - val_loss: 3.3577\n",
      "\n",
      "Epoch 00054: val_loss did not improve from -1.31663\n",
      "Epoch 55/100\n",
      "281/281 [==============================] - 25s 90ms/step - loss: -1.8566 - val_loss: 3.4310\n",
      "\n",
      "Epoch 00055: val_loss did not improve from -1.31663\n",
      "Epoch 56/100\n",
      "281/281 [==============================] - 25s 90ms/step - loss: -1.6932 - val_loss: -0.3518\n",
      "\n",
      "Epoch 00056: val_loss did not improve from -1.31663\n",
      "Epoch 57/100\n",
      "281/281 [==============================] - 26s 91ms/step - loss: -1.8152 - val_loss: -0.0339\n",
      "\n",
      "Epoch 00057: val_loss did not improve from -1.31663\n",
      "Epoch 58/100\n",
      "281/281 [==============================] - 26s 92ms/step - loss: -1.6563 - val_loss: -0.0606\n",
      "\n",
      "Epoch 00058: val_loss did not improve from -1.31663\n",
      "Epoch 59/100\n",
      "281/281 [==============================] - 26s 91ms/step - loss: -1.8577 - val_loss: -0.2435\n",
      "\n",
      "Epoch 00059: val_loss did not improve from -1.31663\n",
      "Epoch 60/100\n",
      "281/281 [==============================] - 26s 91ms/step - loss: -1.7374 - val_loss: -0.9959\n",
      "\n",
      "Epoch 00060: val_loss did not improve from -1.31663\n",
      "Epoch 61/100\n",
      "281/281 [==============================] - 25s 91ms/step - loss: -1.5804 - val_loss: -0.2232\n",
      "\n",
      "Epoch 00061: val_loss did not improve from -1.31663\n",
      "Epoch 62/100\n",
      "281/281 [==============================] - 26s 92ms/step - loss: -1.6641 - val_loss: 5.3808\n",
      "\n",
      "Epoch 00062: val_loss did not improve from -1.31663\n",
      "Epoch 63/100\n",
      "281/281 [==============================] - 25s 89ms/step - loss: -1.6770 - val_loss: 8.8315\n",
      "\n",
      "Epoch 00063: val_loss did not improve from -1.31663\n",
      "Epoch 64/100\n",
      "281/281 [==============================] - 25s 88ms/step - loss: -1.7100 - val_loss: 11.2136\n",
      "\n",
      "Epoch 00064: val_loss did not improve from -1.31663\n",
      "Epoch 65/100\n",
      "281/281 [==============================] - 24s 87ms/step - loss: -1.6380 - val_loss: -0.6833\n",
      "\n",
      "Epoch 00065: val_loss did not improve from -1.31663\n",
      "Epoch 66/100\n",
      "281/281 [==============================] - 24s 87ms/step - loss: -1.7119 - val_loss: -0.6160\n",
      "\n",
      "Epoch 00066: val_loss did not improve from -1.31663\n",
      "Epoch 67/100\n",
      "281/281 [==============================] - 25s 88ms/step - loss: -1.7009 - val_loss: -0.1881\n",
      "\n",
      "Epoch 00067: val_loss did not improve from -1.31663\n",
      "Epoch 68/100\n",
      "281/281 [==============================] - 25s 88ms/step - loss: 1.9677 - val_loss: -0.1848\n",
      "\n",
      "Epoch 00068: val_loss did not improve from -1.31663\n",
      "Epoch 69/100\n",
      "281/281 [==============================] - 25s 89ms/step - loss: -1.2094 - val_loss: 0.9292\n",
      "\n",
      "Epoch 00069: val_loss did not improve from -1.31663\n",
      "Epoch 70/100\n",
      "281/281 [==============================] - 25s 89ms/step - loss: -1.3519 - val_loss: 1.2331\n",
      "\n",
      "Epoch 00070: val_loss did not improve from -1.31663\n",
      "Epoch 71/100\n",
      "281/281 [==============================] - 25s 89ms/step - loss: -1.3587 - val_loss: 5.5836\n",
      "\n",
      "Epoch 00071: val_loss did not improve from -1.31663\n",
      "Epoch 72/100\n",
      "281/281 [==============================] - 25s 89ms/step - loss: -1.3102 - val_loss: 6.0431\n",
      "\n",
      "Epoch 00072: val_loss did not improve from -1.31663\n",
      "Epoch 73/100\n",
      "281/281 [==============================] - 25s 88ms/step - loss: -1.4827 - val_loss: 2.4125\n",
      "\n",
      "Epoch 00073: val_loss did not improve from -1.31663\n",
      "Epoch 74/100\n",
      "281/281 [==============================] - 24s 87ms/step - loss: -1.2498 - val_loss: 1.1542\n",
      "\n",
      "Epoch 00074: val_loss did not improve from -1.31663\n",
      "Epoch 75/100\n",
      "281/281 [==============================] - 24s 87ms/step - loss: -1.4779 - val_loss: -0.5637\n",
      "\n",
      "Epoch 00075: val_loss did not improve from -1.31663\n",
      "Epoch 76/100\n",
      "281/281 [==============================] - 24s 87ms/step - loss: -1.4826 - val_loss: -0.4636\n",
      "\n",
      "Epoch 00076: val_loss did not improve from -1.31663\n",
      "Epoch 77/100\n",
      "281/281 [==============================] - 24s 87ms/step - loss: -1.5174 - val_loss: -0.6533\n",
      "\n",
      "Epoch 00077: val_loss did not improve from -1.31663\n",
      "Epoch 78/100\n",
      "281/281 [==============================] - 24s 87ms/step - loss: -1.6055 - val_loss: -0.3482\n",
      "\n",
      "Epoch 00078: val_loss did not improve from -1.31663\n",
      "Epoch 79/100\n",
      "281/281 [==============================] - 24s 87ms/step - loss: -1.6036 - val_loss: -0.4633\n",
      "\n",
      "Epoch 00079: val_loss did not improve from -1.31663\n",
      "Epoch 80/100\n",
      "281/281 [==============================] - 24s 87ms/step - loss: -1.5360 - val_loss: -0.8597\n",
      "\n",
      "Epoch 00080: val_loss did not improve from -1.31663\n",
      "Epoch 81/100\n",
      "281/281 [==============================] - 24s 87ms/step - loss: -1.4973 - val_loss: -0.8824\n",
      "\n",
      "Epoch 00081: val_loss did not improve from -1.31663\n",
      "Epoch 82/100\n",
      "281/281 [==============================] - 25s 87ms/step - loss: -1.5201 - val_loss: -0.2772\n",
      "\n",
      "Epoch 00082: val_loss did not improve from -1.31663\n",
      "Epoch 83/100\n",
      "281/281 [==============================] - 24s 87ms/step - loss: -1.5842 - val_loss: -0.3428\n",
      "\n",
      "Epoch 00083: val_loss did not improve from -1.31663\n",
      "Epoch 84/100\n",
      "281/281 [==============================] - 25s 88ms/step - loss: -1.5208 - val_loss: 572827.8125\n",
      "\n",
      "Epoch 00084: val_loss did not improve from -1.31663\n",
      "Epoch 85/100\n",
      "281/281 [==============================] - 24s 87ms/step - loss: -1.5189 - val_loss: 9389733888.0000\n",
      "\n",
      "Epoch 00085: val_loss did not improve from -1.31663\n",
      "Epoch 86/100\n",
      "281/281 [==============================] - 25s 87ms/step - loss: -1.6096 - val_loss: 62797892.0000\n",
      "\n",
      "Epoch 00086: val_loss did not improve from -1.31663\n",
      "Epoch 87/100\n",
      "281/281 [==============================] - 25s 87ms/step - loss: -1.6696 - val_loss: 14433517.0000\n",
      "\n",
      "Epoch 00087: val_loss did not improve from -1.31663\n",
      "Epoch 88/100\n",
      "281/281 [==============================] - 24s 87ms/step - loss: -1.5533 - val_loss: 1067304.0000\n",
      "\n",
      "Epoch 00088: val_loss did not improve from -1.31663\n",
      "Epoch 89/100\n",
      "281/281 [==============================] - 25s 88ms/step - loss: 26.3663 - val_loss: 0.0738\n",
      "\n",
      "Epoch 00089: val_loss did not improve from -1.31663\n",
      "Epoch 90/100\n",
      "281/281 [==============================] - 25s 88ms/step - loss: -0.1952 - val_loss: -0.5129\n",
      "\n",
      "Epoch 00090: val_loss did not improve from -1.31663\n",
      "Epoch 91/100\n",
      "281/281 [==============================] - 24s 87ms/step - loss: -0.7723 - val_loss: -0.5889\n",
      "\n",
      "Epoch 00091: val_loss did not improve from -1.31663\n",
      "Epoch 92/100\n",
      "281/281 [==============================] - 25s 87ms/step - loss: -1.0222 - val_loss: -0.4086\n",
      "\n",
      "Epoch 00092: val_loss did not improve from -1.31663\n",
      "Epoch 93/100\n",
      "281/281 [==============================] - 24s 87ms/step - loss: -1.0946 - val_loss: -0.5457\n",
      "\n",
      "Epoch 00093: val_loss did not improve from -1.31663\n",
      "Epoch 94/100\n",
      "281/281 [==============================] - 24s 87ms/step - loss: -1.1817 - val_loss: -0.6207\n",
      "\n",
      "Epoch 00094: val_loss did not improve from -1.31663\n",
      "Epoch 95/100\n",
      "281/281 [==============================] - 24s 87ms/step - loss: -1.2071 - val_loss: -0.4201\n",
      "\n",
      "Epoch 00095: val_loss did not improve from -1.31663\n",
      "Epoch 96/100\n",
      "281/281 [==============================] - 24s 87ms/step - loss: -1.2227 - val_loss: -0.5761\n",
      "\n",
      "Epoch 00096: val_loss did not improve from -1.31663\n",
      "Epoch 97/100\n",
      "281/281 [==============================] - 24s 87ms/step - loss: -1.2086 - val_loss: -0.6099\n",
      "\n",
      "Epoch 00097: val_loss did not improve from -1.31663\n",
      "Epoch 98/100\n",
      "281/281 [==============================] - 24s 86ms/step - loss: -1.2443 - val_loss: -0.8194\n",
      "\n",
      "Epoch 00098: val_loss did not improve from -1.31663\n",
      "Epoch 99/100\n",
      "281/281 [==============================] - 24s 87ms/step - loss: -1.2105 - val_loss: -0.3540\n",
      "\n",
      "Epoch 00099: val_loss did not improve from -1.31663\n",
      "Epoch 100/100\n",
      "281/281 [==============================] - 25s 90ms/step - loss: -1.2474 - val_loss: -0.5119\n",
      "\n",
      "Epoch 00100: val_loss did not improve from -1.31663\n"
     ]
    }
   ],
   "source": [
    "model_name = 'PFN_MDN_STMC_v2'\n",
    "chkpoint = tf.keras.callbacks.ModelCheckpoint('model/' + model_name + '.h5', monitor='val_loss', verbose=1, save_best_only=True)\n",
    "# earlystop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=1e-4, patience=150, verbose=0, restore_best_weights=True)\n",
    "\n",
    "# save history to csv, or to npy later, in order to plot comparison of multiple training runs\n",
    "history_logger = tf.keras.callbacks.CSVLogger('log/' + model_name + '.log', separator=',', append=True)\n",
    "\n",
    "# load previous model if it exists\n",
    "if os.path.isfile('model/' + model_name + '.h5'):\n",
    "    PFN_mdn.load_weights('model/' + model_name + '.h5')\n",
    "\n",
    "history_ct = PFN_mdn.fit(minibatches(data, target, batch_size=1024),\n",
    "        epochs=100,\n",
    "        steps_per_epoch=len(data)//1024,\n",
    "        validation_data=minibatches(val_data, val_target, batch_size=1024),\n",
    "        validation_steps=len(val_data)//1024,\n",
    "        verbose=1,\n",
    "        callbacks=[chkpoint,history_logger])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture output\n",
    "model_name = 'pfn_mdn_regressor_1'\n",
    "chkpoint = tf.keras.callbacks.ModelCheckpoint('model/' + model_name + '.h5', monitor='val_loss', verbose=1, save_best_only=True)\n",
    "# earlystop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=1e-4, patience=150, verbose=0, restore_best_weights=True)\n",
    "\n",
    "# save history to csv, or to npy later, in order to plot comparison of multiple training runs\n",
    "history_logger = tf.keras.callbacks.CSVLogger('log/' + model_name + '.log', separator=',', append=True)\n",
    "\n",
    "# back up\n",
    "# backup  = tf.keras.callbacks.BackupAndRestore(backup_dir=\"backup\")\n",
    "\n",
    "t0 = t.time()\n",
    "history_ct = PFN_mdn.fit(data_generator(data, target, batch_size=1024),\n",
    "        epochs=1000,\n",
    "        steps_per_epoch=len(data)//1024,\n",
    "        validation_data=data_generator(val_data, val_target, batch_size=800),\n",
    "        validation_steps=len(val_data)//800,\n",
    "        verbose=1,\n",
    "        callbacks=[chkpoint,history_logger])\n",
    "t1 = t.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pfn_history = np.load(ext_modelpath + 'pfn_regressor_3f_1.log.npy',allow_pickle='TRUE').item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('PFN_MDN_regressor_weight_nl.log.npy',history_ct.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Time to train: '+str(t1-t0)+' (s)')\n",
    "print('Time to train: '+str((t1-t0)/60)+' (m)')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b5d44aea0cc164fd0ebfc1732b90238ce53cef8d0e631d031394e4c6aaa004a9"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('tensorflow-gpu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
